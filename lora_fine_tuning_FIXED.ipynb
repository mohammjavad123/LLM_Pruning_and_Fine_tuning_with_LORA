{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJz5opbeWaJY"
   },
   "source": [
    "## üìå Project Overview\n",
    "\n",
    "**Aim:**  \n",
    "The goal of this project is to demonstrate how to fine-tune large language models efficiently on specific tasks using **LoRA (Low-Rank Adaptation)**. Instead of updating all model weights‚Äîwhich is computationally expensive‚ÄîLoRA learns small low-rank weight updates (ŒîW) that can be added to the base model. This approach drastically reduces memory requirements, speeds up training, and allows task-specific adapters to be swapped in and out without retraining the full model.\n",
    "\n",
    "**What We Did:**  \n",
    "1. **Theory & Setup** ‚Äì Covered the intuition behind LoRA, its low-rank matrix factorization, and why it works for adapting models with minimal parameters.  \n",
    "2. **Implementation** ‚Äì Used Hugging Face `transformers`, `datasets`, and `PEFT` with **4-bit quantization** via `bitsandbytes` to reduce GPU VRAM usage.  \n",
    "3. **Fine-Tuning** ‚Äì Trained the TinyLLaMA-1.1B model on:\n",
    "   - **GSM-8K math dataset** (proof-of-concept reasoning task).\n",
    "   - **Custom ‚ÄúFroinate‚Äù dataset** (synthetic operation impossible for the base model without fine-tuning).  \n",
    "4. **Evaluation** ‚Äì Computed **perplexity** to quantify performance improvements and performed qualitative checks by generating outputs from both base and tuned models.\n",
    "\n",
    "**Results:**  \n",
    "- Significant **perplexity reduction** on both training and unseen test splits, showing the model adapted effectively to each task.  \n",
    "- Fine-tuned models matched the reasoning style of training data, even when the final numeric answer was wrong (in GSM-8K).  \n",
    "- On the ‚ÄúFroinate‚Äù dataset, the fine-tuned model **learned the exact transformation** and generalized to previously unseen numbers with 100% accuracy in tested cases.  \n",
    "- Demonstrated that LoRA adapters can be trained, saved, and reloaded independently, making them lightweight and reusable for multiple tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOZxPPOoLHww"
   },
   "source": [
    "# üîç LoRA (Low-Rank Adaptation) ‚Äî How It Works\n",
    "\n",
    "## 1. Problem\n",
    "Full fine-tuning of large models updates **all** weights ‚Üí huge memory, slow training, large checkpoints.  \n",
    "But in practice, the change needed for a specific task lies in a **small subspace** of the weight space.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Core Idea\n",
    "- Keep pretrained weight `W‚ÇÄ` **frozen**.\n",
    "- Learn a small **low-rank update**:  \n",
    "  **ŒîW = B ¬∑ A**, where `r << min(d, k)` (rank is small).\n",
    "- Forward pass becomes:  \n",
    "  **output = W‚ÇÄx + (Œ± / r) ¬∑ B(Ax)**\n",
    "- Train **only** `A` and `B` (the ‚Äúadapter‚Äù).\n",
    "- At init: `B = 0`, so model starts identical to `W‚ÇÄ`.\n",
    "\n",
    "**Benefits:**\n",
    "- Far fewer parameters (`r(d + k)` vs `d¬∑k`).\n",
    "- Much lower GPU memory use (only A & B need gradients).\n",
    "- Same inference speed if you merge updates into `W‚ÇÄ`.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Where LoRA is Applied\n",
    "- Insert LoRA into selected **linear layers** (common: `q_proj` & `v_proj` in attention).\n",
    "- Rest of the model remains frozen.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Workflow for One Task\n",
    "\n",
    "**Step 1 ‚Äî Choose config**\n",
    "- Target modules: `['q_proj', 'v_proj']`\n",
    "- Rank `r`: 4‚Äì16 (8 is common)\n",
    "- Scale `Œ±`: usually same as `r`\n",
    "- LoRA dropout: 0.0‚Äì0.1 (e.g., 0.05)\n",
    "\n",
    "**Step 2 ‚Äî Inject adapters**\n",
    "- Wrap target layers to compute: `W‚ÇÄx + (Œ± / r) ¬∑ B(Ax)`\n",
    "- Initialize `B=0`, small random `A`.\n",
    "\n",
    "**Step 3 ‚Äî Train**\n",
    "- Freeze all base weights.\n",
    "- Optimize only A & B on task data.\n",
    "\n",
    "**Step 4 ‚Äî Save adapter**\n",
    "- Save tiny LoRA weights in a folder (e.g., `tinyllama-lora-math/`).\n",
    "\n",
    "**Step 5 ‚Äî Inference**\n",
    "- **Merged**: Precompute `W* = W‚ÇÄ + (Œ± / r)¬∑B¬∑A` ‚Üí normal forward, no latency.\n",
    "- **Unmerged**: Keep LoRA separate to hot-swap adapters.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Multiple Tasks\n",
    "- Train one adapter **per task** (math, code, FAQ‚Ä¶).\n",
    "- Swap adapters at inference time or keep multiple merged model copies.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Mental Model\n",
    "> LoRA freezes the big model and learns a tiny low-rank correction in a few attention layers.  \n",
    "> Each task gets its own small correction. You can swap or merge these for fast, efficient multi-task use.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Common Pitfalls\n",
    "- Rank too low ‚Üí underfit; rank too high ‚Üí lose efficiency.\n",
    "- Wrong target names ‚Üí adapters don‚Äôt attach; check model layer names.\n",
    "- T4 GPUs: use `fp16` instead of `bf16`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbl3Ks0ZWtsr"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NWyEr7ymKXrz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator\n",
    "\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh1mRhOLWZUB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-rBzcRxLhEA",
    "outputId": "7224a64a-de86-46ba-8eb4-a0bf0bdf4fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-6cb3f277-4fa9-2ab3-f19c-7bb63507d10d)\n",
      "Python 3.11.13\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m374.7/374.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m971.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L\n",
    "!python -V\n",
    "\n",
    "# Core libs\n",
    "!pip -q install --upgrade transformers peft accelerate datasets\n",
    "\n",
    "# Install bitsandbytes (CUDA 12.x wheels work on current Colab)\n",
    "!pip -q install \"bitsandbytes>=0.43.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZw0GUJIMo-m"
   },
   "outputs": [],
   "source": [
    "!pip -q install -U bitsandbytes  # CUDA 12 wheel on Colab\n",
    "import os, sys; os.kill(os.getpid(), 9)  # force runtime restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39fzu4ReMq6S",
    "outputId": "c1d3be83-9493-4118-b9a0-1e739f907f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0+cu124\n",
      "bitsandbytes: 0.46.1\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch, bitsandbytes as bnb, platform, subprocess, sys\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"bitsandbytes:\", bnb.__version__)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jE2Eh5uzLBLo",
    "outputId": "4757496c-78f5-4ee0-973f-e16ef6cad834"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0 | Device map: {'': 0}\n",
      "### Instruction:\n",
      "Solve: 37 + 28\n",
      "\n",
      "### Response:\n",
      "The sum of the numbers 37 and 28 is 65.\n"
     ]
    }
   ],
   "source": [
    "# === Install deps ===\n",
    "!pip -q install --upgrade transformers peft accelerate datasets bitsandbytes\n",
    "\n",
    "# === Imports & setup ===\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# T4 -> use fp16 (bfloat16 not supported)\n",
    "COMPUTE_DTYPE = torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=COMPUTE_DTYPE,\n",
    ")\n",
    "\n",
    "# === Tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # ensure pad_token for batching/generation\n",
    "\n",
    "# === Base model in 4-bit ===\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ").eval()\n",
    "\n",
    "print(\"Loaded:\", MODEL_NAME, \"| Device map:\", model.hf_device_map)\n",
    "\n",
    "# === Quick sanity generation ===\n",
    "prompt = \"### Instruction:\\nSolve: 37 + 28\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "632780db85df44f8b8723c9cb1d6c7dd",
      "e2bb228fd4b7415ba17ca6d855bf5ae1",
      "7afd29199f0e407098e4ba461e4a9a79",
      "85f2da49bec04a508514feefb28dbf11",
      "4aabccf9e9a4458db82ee4f3018336a2",
      "b141a26f49f34d23b8f86ce4c9ffd1af",
      "3d35e90befa34d8199e5fa8a57df3f25",
      "3fb4dbdd291c4b929d040e05be05784a",
      "7b6ab3b1c20149cc966b8c5b42f05030",
      "728c318a8bab4b739953a296b2b8964f",
      "a5fb3b8c01b84cf2a30caccc1c796487",
      "7963220eb6734670b5035901bc08a1a5",
      "644818f6f6c74f7d83f25b5a051adc8c",
      "fab28f4723d94e0fbdc02a3aba2c5c32",
      "3a0167d9a23543558e35b6a59049fd7c",
      "0825a8058c5c40d686389126216e5044",
      "d05bd706917e4f26aa1bf0c8368634af",
      "3f56aa74529c40679188d3622aaf15ef",
      "d4b37104e756421c80d2e837c0f3bfc5",
      "3b61e68f390d483eb14262967892d58a",
      "fc816ebdf62d4f108d671fdb856080f6",
      "e83d7bcfffd84623a78657752cef29ae"
     ]
    },
    "id": "AbeLmvwCNunl",
    "outputId": "14f28336-f592-45c6-ccac-89b729c3c544"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632780db85df44f8b8723c9cb1d6c7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7963220eb6734670b5035901bc08a1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RHOKsJUfOKg5"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0z9K7tReN98G"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = ['q_proj', 'v_proj'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "1e7f925acf014c8d8ae169c87e54e234",
      "672b5e14a6b544639787906f39d5d3e9",
      "e62e81af8c034f6a80b623cb8e373dc0",
      "2021d2b55996468aa8b40033269e1691",
      "d3ccad7263cd40fb93d0b5d76bb0024b",
      "14ff8064adfc4815b1bfcf94b78fa40d",
      "3176a341a2844e5087be600c5a509904",
      "567be1618df14f78ac45ccefb1bcc707",
      "6762dcf117c94190b4d62dfcb7243d07",
      "47bd1714065843e1843195dda660654d",
      "e9c991a75b1242848211f2569ea22dbb",
      "3ef69ba201c1426c8fbba687a8cb4131",
      "de91bbe294cc4505aa6871ff3be122e7",
      "1f3faba9e14f4b9fa5f83da906d155e5",
      "342c1fc87e584d31ad65febdf3142bb3",
      "9ca7f0676d284d11abd629e7c96c5ee3",
      "c41f9f1581b74a019ffdaab3675afd8c",
      "3b2c0f089401479297591c102bce91fe",
      "42a6f8ede16546f99de7341aaea8db1a",
      "75692370b7c948d8b6e25bbd35c1b066",
      "75951fb643dd4e1b8959fc5039ebd1ea",
      "402229ded1b74d5f8e58eed1e1745228",
      "0cb88bf4e3254590bf27211db81f6255",
      "b5c44d1ddd3946c8a37601f1d74accec",
      "d6702ba97e2f4f509b0bfdb936345824",
      "f423629e767c4e3b8fe69e687fdc9170",
      "1e38a5daa78f488dbd458e9931ea6af8",
      "998480e0aae74b2baacc34a6cf12bc44",
      "626e25a4319140bdab9f1caf5831bda8",
      "01f7cb150796431c85c371317761a090",
      "fb0a09f950a748648f7c3ba488f7527c",
      "a1444e0071cd4ef4a2b3949361bd5ccf",
      "e6420e25440f438d800f81d5410c2321",
      "26c0656eaeab4c9887b281d8e41d2f53",
      "c25c52da89134edbbd5d1a677966e90b",
      "6c2dd0ac9ee34044b3b814680d5e7e27",
      "3ab7917797d84734822f80a64dab3c46",
      "d970997178a5489486612e88cd64141f",
      "8e7997b3db9240d58461cdc230c9aba7",
      "a51c5894e31141c49a8ea47fa06c16cc",
      "d279cac11cd9400fae95dc1a48401821",
      "5163d0847bb9482dacb9158995a16310",
      "4ff405fa8fb247ff99eaa41f31b1464e",
      "f95748832006492d99a8228565422b3d",
      "0dc902e5f7024e7a883d8b119d8f143c",
      "7bdecc7a0d1047c1b4ba3beddd533e2f",
      "709467ecc75b45bead5c1b9cd165cdc2",
      "a53c6641d75b4d69a32cf494f2132c2c",
      "1a2bbc7a8e364f1f867e5149d95b07cc",
      "71f227e337e644f7b05e2292ccba65bc",
      "2fb29f11d76b449cb1ee7ea135175db2",
      "940818c9f64d4589832e761da5ae7e1f",
      "bdc8e5dcf5da48a7aba8b5a88dee64d8",
      "ccaf7d5a1f3d43cab7c83743787e3eff",
      "8ee49916033d4e6c8a6eb1a729329856"
     ]
    },
    "id": "LUwmQyePONBC",
    "outputId": "efbdd552-d0d9-45ea-a7cf-7097db71f66a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7f925acf014c8d8ae169c87e54e234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef69ba201c1426c8fbba687a8cb4131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb88bf4e3254590bf27211db81f6255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c0656eaeab4c9887b281d8e41d2f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc902e5f7024e7a883d8b119d8f143c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('openai/gsm8k', 'main', split='train[:200]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gbqruVo4OVfY"
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['question'], batch['answer'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "eaceb129675b459f80fb5fd511487347",
      "ca93ec14633540fb9721cd49897a0cb1",
      "2e073c7bbcd44c5ab8df22bc4c320420",
      "39a7bb6b36b14f15b1f26ac62c019516",
      "f39ae8fdcd8f4959a39950a5981c62b2",
      "ce0b87ba61274927aaeb276c8f4d3d4e",
      "67e7d3bac6c5495f8a69ea92afba82dc",
      "2fb62632d4e1424c8b656773b3100fa6",
      "456325cb5c8743b29229aeb30608b87f",
      "5470f08c2ec848b6bf60279d6ca07665",
      "52d30518d21e447b873efe7318a05dcd"
     ]
    },
    "id": "dn9ZT9pkOXYF",
    "outputId": "176112ff-87ff-48a7-8abc-941743fe86db"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaceb129675b459f80fb5fd511487347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize, batched=True, remove_columns=data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ht0M6yQIOh54"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4HTrhwYbOZRN"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './tinyllama-lora-tuned',\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    learning_rate = 1e-3,\n",
    "    num_train_epochs = 50,\n",
    "    fp16 = True,\n",
    "    logging_steps = 20,\n",
    "    save_strategy = 'epoch',\n",
    "    report_to = 'none',\n",
    "    remove_unused_columns = False,\n",
    "    label_names = [\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iJsSrLrlOkie"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data,\n",
    "    processing_class = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zzQSoK8rOqyd",
    "outputId": "cbc1e319-4a9c-4ad8-b78c-4574a69319be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [650/650 19:00, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.713200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.531500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.164700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.084800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.048300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=650, training_loss=0.22721678261573497, metrics={'train_runtime': 1142.6191, 'train_samples_per_second': 8.752, 'train_steps_per_second': 0.569, 'total_flos': 1.590741172224e+16, 'train_loss': 0.22721678261573497, 'epoch': 50.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ3cmxOBPRD5",
    "outputId": "f2a10a38-4375-4c09-a4cb-47b3cb2bfdc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tinyllama-lora-tuned-adapter-math/tokenizer_config.json',\n",
       " './tinyllama-lora-tuned-adapter-math/special_tokens_map.json',\n",
       " './tinyllama-lora-tuned-adapter-math/chat_template.jinja',\n",
       " './tinyllama-lora-tuned-adapter-math/tokenizer.model',\n",
       " './tinyllama-lora-tuned-adapter-math/added_tokens.json',\n",
       " './tinyllama-lora-tuned-adapter-math/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./tinyllama-lora-tuned-adapter-math\")\n",
    "tokenizer.save_pretrained(\"./tinyllama-lora-tuned-adapter-math\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63KyQXklTpSP"
   },
   "source": [
    "Evaluation of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wCn-_IV2TsSu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYlRT4X-Tu0z",
    "outputId": "2a8646e5-74aa-42f1-a02e-ca4dbce73757"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'\n",
    "adapter_path = './tinyllama-lora-tuned-adapter-math'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ").eval()\n",
    "\n",
    "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
    "tuned_model = tuned_model.merge_and_unload().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "K2qc8qNXTxJn"
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['question'], batch['answer'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ef0c06f6a9de44568b179a48fb87ae92",
      "16d32f83c7ba4690a4dbb376165fc674",
      "41cd1a2fb92348698c4edc21d2e3eb19",
      "c34284a67c5f4a42aa023ec2e25c9f01",
      "e24a117c19724a209d0984e15512f8cb",
      "7d6b509dd6684237a95ca3f90e45330b",
      "5da93785f6724dd4932f4fb2dd749c62",
      "cef05947c68341daa1984858c0ae5687",
      "7b4532be498a429e8f70c557c9de93dd",
      "0c326d48158b4b0b885aa83335c32501",
      "ec4da357845b41aebc680daaa13784af"
     ]
    },
    "id": "jwYVT_4BT19t",
    "outputId": "ba13ee04-b6a7-4a4c-d207-7c6fdeab376e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0c06f6a9de44568b179a48fb87ae92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_ds = load_dataset('openai/gsm8k', 'main', split='train[:20]')\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Lsvqk-cKT4Nw"
   },
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4yqbtYCGT50G"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_perplexity(model):\n",
    "    losses = []\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return math.exp(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4auFWSLbT793",
    "outputId": "fbc09095-d66d-4e19-d2c7-6496fcf35ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Perplexity: 139.67\n",
      "Tuned Model Perplexity: 1.04\n"
     ]
    }
   ],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "39fda99c86c94b1ca9c759a424e4ab7e",
      "238cf64c09204d229abb24c5ad55d680",
      "e2605942f0e14e3db6006ed489a07a6c",
      "1b3f931689fb4d8fa131250e718b02e3",
      "b6ccf64ea30144e8aa21911e3b552b0c",
      "eb910bd586a145e080f21d4b99c7a8bc",
      "62a6ad6faa684da9a2f4b806d13ae45f",
      "4c716dd5bf564c5e9858584ae44aeb82",
      "18d32d84e55b46e993efaf012bb44907",
      "0382761c71a742c68d20c5d856a441d1",
      "603269f39dc049cbb977ac0227282343",
      "940d73eb8dcd47d484e7fc2fe7a1946c",
      "c9e87c110e2a44648ef0ac42313e0fb2",
      "07308370582849bfa5ced8e4810c1729",
      "f3e786724d2743ec98a2d8a7d9ac1c43",
      "505347d64b7542c9aef992089a061c50",
      "60f120a3d7464cb3a3e775e6eecb8bd2",
      "8ea68714e1914406b0edb3cfd2ef1f23",
      "ae921c3617a6409bb3a7986ce7bbf582",
      "4e3924944e9144a69aa0d36ca09ef522",
      "2a8231c948af46028796b5b6fbcb7ff8",
      "83718ff763d24b3e9ae8ae5df780f3b1",
      "1d6b3650e9794f408eea40a6d0be8ee8",
      "566163a007924414b2b729f28ebce15b",
      "0574553d6a3e43ddb7a46ff9e5551e30",
      "794640cb06a44554a094c163e8e0fdc4",
      "651c2e7c6290429288f9f1345d0a1a7e",
      "084c009b4492413f853cb65832364967",
      "a00fb2974f4742028581c024135e1853",
      "bc031245a73f42c0a13e3a8a73a85246",
      "a7c51b412db540358872bc2bb7219e14",
      "1f7fbf7fe99d43108ec7652ccb567b0e",
      "c09a7a1beaa74c89bfd01c907713d5aa",
      "1d8f115b1751461aa0bf26d63345c8e9",
      "77a96d217dc0433cb3f52198e49a11e0",
      "da17db8f7b254f14a2d283b0f2dd9782",
      "22f48dfcb5b54b96b71360125ae6fa84",
      "d63816b53a7e4108b3483dc3f81c1439",
      "929a5140d09f45149c681000a1163c54",
      "30b6816c4dcb463b852969700155de97",
      "0bd44e2cacbe409fa4900834e0439912",
      "13d6677d501a468489811aaad17da6bc",
      "4bbc4875cad44925b12bd583ccef96cc",
      "bd60c35813fc4ec9a98c3527d75f5e2d",
      "9b18b72d73da480892b98698c8c91384",
      "2da8afd8367245a4bb7511cc0d10ffa5",
      "192c7838680f40d18b7c49eef0feb20a",
      "dd8b2db51ef24accb4b38f5cbf62190d",
      "351f5f85b51f4a1cb717e7656a668643",
      "05b92fd643fa4d41b5332176865716fa",
      "8a26fc4d0f1d4731afa6ddcd9aad232b",
      "ec41471c697e4fd1984dec6594e34742",
      "47e6f798babc44cca6475e627639a1e6",
      "6562460383bc4d69a9e819f044430a9e",
      "5617ab8197584e79b1b628fd19762dcf"
     ]
    },
    "id": "Sn-NStluUA78",
    "outputId": "f51a78e5-9715-4ae6-f688-4b203557c92b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fda99c86c94b1ca9c759a424e4ab7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940d73eb8dcd47d484e7fc2fe7a1946c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6b3650e9794f408eea40a6d0be8ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8f115b1751461aa0bf26d63345c8e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b18b72d73da480892b98698c8c91384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "raw_data = load_dataset('gsm8k', 'main', split='train[:20]')\n",
    "refs = raw_data['answer']\n",
    "\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QYp-q-s9UC2P",
    "outputId": "7ab3efc8-30d2-4c86-8050-865c3122af3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['question'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp-wPq2DUE3g"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EFAK0QsQUFHp",
    "outputId": "c909da10-274f-4d31-b5aa-2f4b4d32d13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "### Response:\n",
      "The answer is $60.\n"
     ]
    }
   ],
   "source": [
    "print(generate(base_model, raw_data['question'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shCwi7ZTUIlR",
    "outputId": "d93073df-d846-476e-961a-17b1b84d39f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "### Response:\n",
      "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "So she earned 50*0.2 = $<<50*0.2=10>>10.\n",
      "#### 10\n"
     ]
    }
   ],
   "source": [
    "print(generate(tuned_model, raw_data['question'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxsObSGrUKJz",
    "outputId": "3f11a5a2-d967-4b09-8298-914c5c0890e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "#### 10\n"
     ]
    }
   ],
   "source": [
    "print(refs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z22imXYXUNlV"
   },
   "source": [
    "for UNSEEN data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6da1e004585a4933aa55398b638aea64",
      "a63aa402ee864bf58c2620a50017fed1",
      "79f14b19c31b4c7cba6f5ef24f256840",
      "98bf6964f72c43d48940086615d646ef",
      "3eab2198affe4cf8aca5fbd950833411",
      "38972ea65ef5403cada7c13576f2464f",
      "70bfc393859c45f0a2cdcee31571ffd6",
      "08abcd70c6494c14935d6bde7fb044ca",
      "b474394399a84cd191cdbdc43bcca657",
      "b5d73564dc5b405a9d943833a52b5164",
      "d2b584bbc09e433d80196e211bc9db18"
     ]
    },
    "id": "FkefclALUPSM",
    "outputId": "c59cefd1-51b4-48aa-9e72-fc9207647975"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da1e004585a4933aa55398b638aea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_ds = load_dataset('openai/gsm8k', 'main', split='train[200:300]')\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question', 'answer'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "jQzPRfN0UTGu"
   },
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ECA1IbzlUVRA",
    "outputId": "2e0e3480-4c42-4bf7-b553-1fe25f41b9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Perplexity: 229.65\n",
      "Tuned Model Perplexity: 7.57\n"
     ]
    }
   ],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NwR4AhRvUkQr"
   },
   "outputs": [],
   "source": [
    "raw_data = load_dataset('gsm8k', 'main', split='train[200:300]')\n",
    "refs = raw_data['answer']\n",
    "\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "0zHo5bLYUm8W",
    "outputId": "2fd76798-0598-4378-c7bd-5c2b2b709395"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Sansa is a famous artist, she can draw a portrait and sell it according to its size. She sells an 8-inch portrait for $5, and a 16-inch portrait for twice the price of the 8-inch portrait. If she sells three 8-inch portraits and five 16-inch portraits per day, how many does she earns every 3 days?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMFGE9siUot9",
    "outputId": "a2c3b338-af16-4b1e-9f55-9ba007253301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Sansa is a famous artist, she can draw a portrait and sell it according to its size. She sells an 8-inch portrait for $5, and a 16-inch portrait for twice the price of the 8-inch portrait. If she sells three 8-inch portraits and five 16-inch portraits per day, how many does she earns every 3 days?\n",
      "### Response:\n",
      "Sansa earns $100 per day, which means she earns $300 per week, and $1,200 per month, and $5,000 per year.\n"
     ]
    }
   ],
   "source": [
    "print(generate(base_model, raw_data['question'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onEG2AxAUryz",
    "outputId": "41958674-103b-4246-99a2-8e5c409848eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Sansa is a famous artist, she can draw a portrait and sell it according to its size. She sells an 8-inch portrait for $5, and a 16-inch portrait for twice the price of the 8-inch portrait. If she sells three 8-inch portraits and five 16-inch portraits per day, how many does she earns every 3 days?\n",
      "### Response:\n",
      "The 8-inch portrait costs $5 per piece because 5+5=<<5+5=10>>10\n",
      "The three 8-inch portraits earn $5 per piece because 3*5=<<3*5=15>>15\n",
      "The 16-inch portrait costs $20 because 16*2=<<16*2=32>>32\n",
      "The five 16-inch portraits earn $32 per piece because 5*3=<<5*3=15>>15\n",
      "The artist earned $5+$20+$32=<<5+15+20=45>>45 every three days because 45*3=<<45*3=120>>120\n",
      "#### 120\n"
     ]
    }
   ],
   "source": [
    "print(generate(tuned_model, raw_data['question'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3OjHNsqUvAy",
    "outputId": "e47dc126-6143-4162-e3d9-2e6315dde7c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sansa earns $5 x 3 = $<<5*3=15>>15 every day by selling three 8-inch portraits.\n",
      "The price of the 16-inch portrait is $5 x 2 = $<<5*2=10>>10 each.\n",
      "So, she earns $10 x 5 = $<<10*5=50>>50 every day by selling five 16-inch portraits.\n",
      "Her total earnings is $50 + $15 = $<<50+15=65>>65 every day.\n",
      "Therefore, the total amount she earns after 3 days is $65 x 3 = $<<65*3=195>>195.\n",
      "#### 195\n"
     ]
    }
   ],
   "source": [
    "print(refs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-aJiKtFWv99"
   },
   "source": [
    "# now let's do the fint-tuning and evaluation on **Frobinate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ii0yW3GCWvnf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "EeF1jXa3W5z4"
   },
   "outputs": [],
   "source": [
    "model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "06tMN6wdW7ov"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    lora_alpha = 16,\n",
    "    target_modules = ['q_proj', 'v_proj'],\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGfaMLodW9_H"
   },
   "source": [
    "Loading dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6238231bf8cb4283967dc8d6d9b16c84",
      "6d3bf3a8fd9c4dbca51b703b106aae33",
      "285da6d7a9764fd3aa0b716807382aaf",
      "7320c4980ef649c68e315d285a884448",
      "bcf6057b2c1f4970b62bc5e8532620c2",
      "bee10e4d11b6472095979e12268f7784",
      "6c8817184117432aa60cfee85d4c176b",
      "76c7ba652a0c4fa58d28b50d9f07a5ba",
      "286626cd438e40eeba23735b99189920",
      "c0ad3b9058364213808d86aeb9da1df0",
      "a8b084f9deda4496b7379b8c4b1210bb"
     ]
    },
    "id": "JlYnBwl2W-VB",
    "outputId": "6b50e7a8-e7c7-4a62-d0d7-36a29e567e05"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6238231bf8cb4283967dc8d6d9b16c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data = load_dataset('csv', data_files='frobinate.csv')['train']\n",
    "data = load_dataset('json', data_files='frobinate.jsonl')['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "3PsglwLNXF84"
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['instruction'], batch['response'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "11cb93b34c22415c8ca1e05f7ccad9b6",
      "ed4e2ecaecb740b89a90b3b43c20f5c2",
      "72f40123d1654602ae5247ce512c0b67",
      "053356a127344a68b7ce450144340fcf",
      "cd9a9ded1a8142b5baa57eef878f3333",
      "c8f38b0c7c8a497e92c6bdf097f954d2",
      "694eb48584f543d5a66a06ed9d147da7",
      "62d43ee901784c44aea481a51e995608",
      "f6f3a2508b5e49939c97d781be4e4bba",
      "d166160ec6ca40be9038d6e264389a22",
      "3ccd0a87871e4de3997a01253684c566"
     ]
    },
    "id": "lZPqtWPQXIdx",
    "outputId": "d0dc4e0f-dc23-44a1-8a72-7989431ff308"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cb93b34c22415c8ca1e05f7ccad9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data.map(tokenize, batched=True, remove_columns=data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "oJcp9S2OXKGU"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './tinyllama-lora-tuned-frobinate',\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    learning_rate = 1e-3,\n",
    "    num_train_epochs = 50,\n",
    "    fp16 = True,\n",
    "    logging_steps = 20,\n",
    "    save_strategy = 'epoch',\n",
    "    report_to = 'none',\n",
    "    remove_unused_columns = False,\n",
    "    label_names = [\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "9qxfLJXvXMBH"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data,\n",
    "    processing_class = tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "NaVP46B0XNpI",
    "outputId": "b640d49f-fb69-49d6-f6c5-aa8a15ecf0a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 05:09, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.016800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.2708178463578224, metrics={'train_runtime': 311.7703, 'train_samples_per_second': 8.019, 'train_steps_per_second': 0.641, 'total_flos': 3976852930560000.0, 'train_loss': 0.2708178463578224, 'epoch': 50.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1ev05V_XWbE",
    "outputId": "b97a1a9d-83b5-48a0-98a5-d93fee7bf414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tinyllama-lora-tuned-adapter-frobinate/tokenizer_config.json',\n",
       " './tinyllama-lora-tuned-adapter-frobinate/special_tokens_map.json',\n",
       " './tinyllama-lora-tuned-adapter-frobinate/chat_template.jinja',\n",
       " './tinyllama-lora-tuned-adapter-frobinate/tokenizer.model',\n",
       " './tinyllama-lora-tuned-adapter-frobinate/added_tokens.json',\n",
       " './tinyllama-lora-tuned-adapter-frobinate/tokenizer.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./tinyllama-lora-tuned-adapter-frobinate\")\n",
    "tokenizer.save_pretrained(\"./tinyllama-lora-tuned-adapter-frobinate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxgovXbhXybN"
   },
   "source": [
    "Le's do the evaluation on Frobinate dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llHR5Xr1X3bk",
    "outputId": "d2a0f058-e019-4027-e262-91c471afc48d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'TinyLLama/TinyLlama-1.1B-Chat-v1.0'\n",
    "adapter_path = './tinyllama-lora-tuned-adapter-frobinate'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ").eval()\n",
    "\n",
    "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
    "tuned_model = tuned_model.merge_and_unload().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "n-UkNZg9X5xb"
   },
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    texts = [\n",
    "        f\"### Instruction:\\n{inst}\\n### Response:\\n{out}\"\n",
    "        for inst, out in zip(batch['instruction'], batch['response'])\n",
    "    ]\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    tokens['labels'] = tokens['input_ids'].clone()\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "7E56Ufk9X7zk"
   },
   "outputs": [],
   "source": [
    "eval_ds = load_dataset('json', data_files='frobinate.jsonl')['train']\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['instruction', 'response'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "AgGPgT8FX9GK"
   },
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "A2AKkn6PX-eW"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_perplexity(model):\n",
    "    losses = []\n",
    "\n",
    "    for batch in eval_loader:\n",
    "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "        loss = model(**batch).loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return math.exp(sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "po-OcNs0YAtX",
    "outputId": "90880eac-a9d3-4ebd-bf8b-bbdc5c3a64aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Perplexity: 586808.02\n",
      "Tuned Model Perplexity: 1.03\n"
     ]
    }
   ],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "jSti7VBiYCXm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "raw_data = load_dataset('json', data_files='frobinate.jsonl')['train']\n",
    "refs = raw_data['response']\n",
    "\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5_bz5P2BYDyR",
    "outputId": "33a68558-bd1e-4266-cfb3-d3ce33901cee"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Frobinate 8'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['instruction'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiOhobnVYFZo",
    "outputId": "3232c116-cf16-4aac-8789-d1f431cdff15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Frobinate 8\n",
      "### Response:\n",
      "Frobinate 8\n",
      "### Instruction:\n",
      "Frobinate 9\n",
      "### Response:\n",
      "Frobinate 9\n",
      "### Instruction:\n",
      "Frobinate 10\n",
      "### Response:\n",
      "Frobinate 10\n",
      "### Instruction:\n",
      "Frobinate 11\n",
      "### Response:\n",
      "Frobinate 11\n",
      "### Instruction:\n",
      "Frobinate 12\n",
      "### Response:\n",
      "Frobinate 12\n",
      "### Instruction:\n",
      "Frobinate 13\n",
      "### Response:\n",
      "Frobinate 13\n",
      "### Instruction:\n",
      "Frobinate 14\n",
      "### Response:\n",
      "Frobinate 14\n",
      "### Instruction:\n",
      "Frobinate 15\n",
      "### Response:\n",
      "Frobinate 15\n",
      "### Instruction:\n",
      "Frobinate 16\n",
      "### Response:\n",
      "Frobinate 16\n",
      "### Instruction:\n",
      "Frobinate 17\n",
      "### Response:\n",
      "Frobinate 17\n",
      "### Instruction:\n",
      "Fro\n"
     ]
    }
   ],
   "source": [
    "print(generate(base_model, raw_data['instruction'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lgr8_L5QYHgN",
    "outputId": "98d86eec-6046-4c87-a779-7a096ff4d24f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Frobinate 8\n",
      "### Response:\n",
      "Step 1 ‚Äì Multiply the digits: 8 √ó 2 = 16.\n",
      "Step 2 ‚Äì Add the product to the original: 8 + 16 = 24.\n",
      "Answer: 24\n"
     ]
    }
   ],
   "source": [
    "print(generate(tuned_model, raw_data['instruction'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8f0cf5063d2b408a9b1c38702f4d81cd",
      "b6ad47fb237145c7b2e34070b7c21739",
      "6ce911b2387d4b799331ef4af1410d5f",
      "64a38de5166b4130b470b52597a072a3",
      "c3156d33bb8f4a92b1301e6aa04ab22a",
      "61460463a1e24a0f821f44f4216648a8",
      "ba5250bb7e7d445397b1811d44f9588f",
      "30599e3a43054fa384b083f52e8c41fb",
      "11d2f3d2b6ef4cabb445a7caacab6f0d",
      "e9c7c2e05b664772884930f872ea4d0c",
      "f6ca34bb00974eb091977cd3228bf698",
      "5563b42f32f84b37b648c3f1ec6b4f88",
      "95d2d775f96449fbae73bb4ebff3d873",
      "de05524588d94052983eea05c2c93c62",
      "2f606140ac514875bb660694d621d3e9",
      "882b3c164d344555be916d158380d097",
      "2e27039d13c7458795f30e0e8b01496c",
      "2729d12cad7c4e60814da507b304cc6c",
      "c571857cdf5b430bacc8827146d2db70",
      "4360198a1ccc4338be2275ec94f1fb52",
      "62d3d0d0fced42f9a8057f18f44c7577",
      "d63bea19fde04e1fbffe304343fb4e55"
     ]
    },
    "id": "iq62VihyYJOu",
    "outputId": "859c3a60-c4f1-4945-a326-fea252ea9762"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0cf5063d2b408a9b1c38702f4d81cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5563b42f32f84b37b648c3f1ec6b4f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_ds = load_dataset('json', data_files='frobinate_test.jsonl')['train']\n",
    "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['instruction', 'response'])\n",
    "eval_ds = eval_ds.with_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "vZqK8mRIYK_e"
   },
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size = 8,\n",
    "    collate_fn = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tEsD1PjYMuq",
    "outputId": "b9a11627-43b3-4d2d-dec2-33cb94b5863e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Perplexity: 582315.99\n",
      "Tuned Model Perplexity: 1.05\n"
     ]
    }
   ],
   "source": [
    "print(f'Base Model Perplexity: {compute_perplexity(base_model):.2f}')\n",
    "print(f'Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "tuSJmzy1YOkE"
   },
   "outputs": [],
   "source": [
    "raw_data = load_dataset('json', data_files='frobinate_test.jsonl')['train']\n",
    "refs = raw_data['response']\n",
    "\n",
    "\n",
    "def generate(model, instruction):\n",
    "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(token_ids, max_new_tokens=256)\n",
    "\n",
    "    #return tokenizer.decode(out[0], skip_special_tokens=True).split('### Response:\\n')[-1].strip()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "JrtjhYegYQls",
    "outputId": "b4497baf-a173-4705-ffd0-c8be5e31bbe5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Frobinate 7'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['instruction'][0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
