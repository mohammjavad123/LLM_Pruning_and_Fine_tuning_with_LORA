{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_uDsYeoAagi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-Q5DKtZATP9"
   },
   "source": [
    "# 🦙 TinyLlama Pruning & Summarization Experiment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this experiment, we explore the effects of pruning and fine-tuning on a large language model (TinyLlama) for the task of text summarization using the CNN/DailyMail dataset. The workflow is as follows:\n",
    "\n",
    "1. **Dataset Preparation**:  \n",
    "   We selected a subset of the CNN/DailyMail summarization dataset, pairing news articles with their human-written summaries.\n",
    "\n",
    "2. **Base Model Evaluation**:  \n",
    "   We loaded the pretrained TinyLlama model and evaluated its ability to summarize articles using standard prompts and ROUGE metrics. As expected, the base model (not fine-tuned) produced mostly extractive or paraphrased outputs with low ROUGE scores.\n",
    "\n",
    "3. **Pruning with SparseGPT**:  \n",
    "   Using the SparseGPT algorithm, we pruned the TinyLlama model to 50% sparsity, leveraging a calibration set drawn from the same dataset. Pruning was done in-place on all linear layers, guided by actual activations collected via forward hooks.\n",
    "\n",
    "4. **Pruned Model Evaluation**:  \n",
    "   We evaluated the pruned model and observed that ROUGE scores remained similar to the base model, as neither had been fine-tuned for summarization.\n",
    "\n",
    "5. **Next Steps: Fine-Tuning**  \n",
    "   With the pruned model, we are ready to fine-tune on our summarization dataset. Fine-tuning will enable the model to generate more accurate and abstractive summaries. After fine-tuning, we will compare ROUGE scores and sample outputs before and after pruning to understand the trade-offs between model compression and task performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "51EjQgdJ86Ha",
    "outputId": "25fab84b-5786-403a-fe8c-5c7f4d876e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: git+https://github.com/locuslab/wanda.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q transformers accelerate bitsandbytes datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "b72f02839e4b4c6c9b10e7a553a73ac1",
      "20e85d3412ee41c7a462e12104f91735",
      "71b5ff04963341bba388aa924edc6e4f",
      "27514544e1ef4aa99feb7f92bf097b80",
      "71ed8f7a893346a082bbd6fbd23708fa",
      "bc1c450e69f146d0a72e6f12bdc17bf6",
      "49199f994390492da2f01d0e3f15b26f",
      "24ba3352595240d9956cac8edd0190f7",
      "4859ae6d802c456ab1b1f42214319eee",
      "4ce67fd1851a4b81aab7b6703e9ab7d0",
      "72e084625a3442eb9df4354242d85c17",
      "d64f9a575b02491fb13f85c51adf8df7",
      "757b1833d67043b9b516dcb7dac81658",
      "dcb2a20481b24aeca6486cd01411c292",
      "8f443abd963d4d0f840bb61c0b78d138",
      "cb2f99a3af014445bfcacb34dd9baadf",
      "8ffd1c6f9f374966a616c43ee400ae90",
      "2adc96d650ba4436a59f150e6f04a365",
      "54f8fa79440a4c5580c77d9a998b0205",
      "6c7bd386238345ddac401e0f6d92ab76",
      "9cd51e90c01c4d909bcd1285f6fc2636",
      "7e41ecd7ed5542f38aaf2a2a4b7c1b99",
      "1c281867e99f4dd09bfb32dafe819edf",
      "50900776ba86497bb1dbf88a8fb4dfc2",
      "7a89d2ae321f4712917e12184271c5b3",
      "4cb141b2f5864deb81ba89cf115207f0",
      "cc94a0bf2f9440e7a24a2346dc143c57",
      "5cd765847314442db73ec8756f74ce25",
      "ae33b3567000408fa8976de3242c9f58",
      "403a66f348f64fc5b14488bea835806c",
      "89ce8a5088fe4a88af1ada7722ecbc18",
      "9b1eee49f5af4cbf8ff2d276fa503a49",
      "bc5769ee9f9f4239b2fbf367ff0d5a90",
      "5db85c667ab6447e9391b7938e9d49e7",
      "2c765780546f4ed7a34e4f293291fc2b",
      "98710a39ec2e4d5d81b3b8eb9033c932",
      "863d8c6293dc4e59aba79a67684b7c3d",
      "06e43023ed504fa0824c72bc782fef16",
      "305bd8c34c0043c99bec109811b027a9",
      "cdb05b8333b04bc8a1471cfe5fa44ba3",
      "2a09267b46a442798b33fadafd8df8cd",
      "6ceeae60ebce44bb9e3df6311c4aeb9c",
      "ea7fb31cf055409787d0ab494adaa2ae",
      "c4e78ec0ca784b579574f3a36f948579",
      "11511fed4ffa4750a8006c86f3629b05",
      "96505c5a30a04904b494d4a504b46cff",
      "5669820ebe71473692bc3f5eed53b90c",
      "28e60c932a954791a00a709446cd5a3c",
      "768759b62ca541429624392b93a52474",
      "fb8ea2e5892c44529f761f34083d9174",
      "7215895bdbe64246a54d96cacd2d3268",
      "58007f04035b4b87bb9f295d87fee20e",
      "a9d2740face44854815a234552a8a6b7",
      "8994b2fc8ea341199874a1bc0dd39ad2",
      "4aa7d228a7d14a8aae646e4c291af229",
      "9ca4ecd25a114c768ddd32063e949c65",
      "803cb1472944474aa2756a3f95de995d",
      "1f6b867cd7f041e5925ebd5cd42f481c",
      "5495d885edff45c8b7ffcb4b8a3f1ebd",
      "606f22993a0a4103a235e8c95e460323",
      "ca2ca492e7e84955bf9a060b88985561",
      "143ce5c0fc2e4c8cbe477f7f4736d2b1",
      "50789e8575f04325b59557a9d9b6ff17",
      "050b0b0cd7e84b59b9b1c20a454c64c4",
      "41e4f8e568574c34ae8ad50813ac6be5",
      "a3b1b87da7fd4dd2b6f706e3e048c689",
      "5b3d405500ea40c2a9de8d2aa9a78610",
      "61f4574162a74d7f86920142e2e00025",
      "507ad3cac5bb416faebe7302ae438597",
      "65b1e239f44e47fe809ae818025769bb",
      "a0592e7026204ccea97afe73ba86cc3f",
      "2584b279ce4949b6b86ebdf7be578f38",
      "ab6a3f22342a46dab28494359c729af6",
      "a5bf6f624b4d4c638fa846d559f156ac",
      "50e8b1ff3cda47a0bd45453ab95b70e9",
      "dde32669fa0b4c35b477e8962178118b",
      "5bc7a567d3cf4992a1086984f20e1cb9"
     ]
    },
    "id": "S_CBmICnXELw",
    "outputId": "0b688652-d0bd-48d8-c82f-ea008e0cdad3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72f02839e4b4c6c9b10e7a553a73ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64f9a575b02491fb13f85c51adf8df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c281867e99f4dd09bfb32dafe819edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db85c667ab6447e9391b7938e9d49e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11511fed4ffa4750a8006c86f3629b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca4ecd25a114c768ddd32063e949c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3d405500ea40c2a9de8d2aa9a78610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1RJjSfsXvKP"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-0r1i4kCZt2j",
    "outputId": "75fa7dc3-9e47-4a17-9710-633f30d979e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/424.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/424.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.6/424.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers optimum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M33bK2VM4AwS"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84KZCsQ6DLB_",
    "outputId": "46dfe1cc-2dbe-4a9f-fdab-df74a43b8bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SparseGPT'...\n",
      "remote: Enumerating objects: 46, done.\u001b[K\n",
      "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
      "remote: Total 46 (delta 22), reused 10 (delta 10), pack-reused 14 (from 2)\u001b[K\n",
      "Receiving objects: 100% (46/46), 26.80 KiB | 13.40 MiB/s, done.\n",
      "Resolving deltas: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/IST-DASLab/SparseGPT.git\n",
    "import sys\n",
    "sys.path.append('/content/SparseGPT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLOosmOQD4cU"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers einops sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_mvr5pOD7c5"
   },
   "outputs": [],
   "source": [
    "from sparsegpt import SparseGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekf0sXE8aE2Q",
    "outputId": "cfb26fdf-8f2b-4964-f609-970ccad4a37a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwwsfCE-aG3P"
   },
   "outputs": [],
   "source": [
    "# Calibration prompts for pruning\n",
    "calib_prompts = eval_df['text'].sample(32, random_state=42).tolist()\n",
    "\n",
    "# All texts for evaluation\n",
    "eval_prompts = eval_df['text'].tolist()\n",
    "\n",
    "# (Optional) Corresponding true labels for evaluation\n",
    "eval_labels = eval_df['label'].tolist() if 'label' in eval_df.columns else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQAEMJRcEfuN"
   },
   "outputs": [],
   "source": [
    "def get_calib_data(tokenizer, prompt_list, batch_size=4):\n",
    "    for i in range(0, len(prompt_list), batch_size):\n",
    "        batch = prompt_list[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).input_ids.cuda()\n",
    "        yield tokens\n",
    "\n",
    "calib_data = list(get_calib_data(tokenizer, calib_prompts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2j6zUiJEtO8"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaUsurmeEuh_"
   },
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device  # ← safest way to get device\n",
    "\n",
    "for text in eval_prompts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=16)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    baseline_outputs.append(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoP0UOuWE6jS"
   },
   "outputs": [],
   "source": [
    "def get_calib_data(tokenizer, prompt_list, batch_size=4, device=\"cuda\"):\n",
    "    for i in range(0, len(prompt_list), batch_size):\n",
    "        batch = prompt_list[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        yield tokens\n",
    "\n",
    "calib_data = list(get_calib_data(tokenizer, calib_prompts, device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "n5tz-yOUE9MK",
    "outputId": "bbbbe9d2-ba9a-4fd3-f901-4db890c4f4f5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-21-2438133163.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbaseline_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_prompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "baseline_outputs = []\n",
    "for text in eval_prompts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=16)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    baseline_outputs.append(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "rqEZFCI5EmVR",
    "outputId": "1831f10f-62e6-43ba-86e2-693ad9d26c3a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-20-881418574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalib_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_calib_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_df' is not defined"
     ]
    }
   ],
   "source": [
    "calib_prompts = eval_df['text'].sample(32, random_state=42).tolist()\n",
    "\n",
    "def get_calib_data(tokenizer, prompt_list, batch_size=4, device=\"cuda\"):\n",
    "    for i in range(0, len(prompt_list), batch_size):\n",
    "        batch = prompt_list[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).input_ids.to(device)\n",
    "        yield tokens\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "calib_data = list(get_calib_data(tokenizer, calib_prompts, device=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "pWX1i2d2Fu8q",
    "outputId": "609f1fc4-9174-4d6a-81da-cca65cfd8de1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calib_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-19-974559039.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcalib_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_calib_data_fixed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalib_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalib_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'calib_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "calib_seq_len = 64  # Choose what makes sense for your data/model\n",
    "\n",
    "def get_calib_data_fixed(tokenizer, prompt_list, batch_size=4, seq_len=64, device=\"cuda\"):\n",
    "    for i in range(0, len(prompt_list), batch_size):\n",
    "        batch = prompt_list[i:i+batch_size]\n",
    "        # Always pad & truncate to the same length\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=seq_len).input_ids.to(device)\n",
    "        yield tokens\n",
    "\n",
    "calib_data = list(get_calib_data_fixed(tokenizer, calib_prompts, seq_len=calib_seq_len, device=device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNJgadwTHJJM"
   },
   "source": [
    "#  LLM Pruning with SparseGPT: State-of-the-Art Transformer Compression\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this section, we perform *one-shot* pruning of a large language model (LLM) using [SparseGPT](https://arxiv.org/abs/2301.00774), a highly efficient and research-proven algorithm for reducing LLM size and inference cost.  \n",
    "Our workflow demonstrates how to compress a HuggingFace transformer (e.g., TinyLlama) on the AG News dataset while retaining high accuracy—enabling real-world deployment of LLMs on limited hardware.\n",
    "\n",
    "---\n",
    "\n",
    "##  Why Prune LLMs?\n",
    "\n",
    "Transformer-based LLMs, such as GPT, LLaMA, and TinyLlama, have hundreds of millions or even billions of parameters.  \n",
    "**Pruning** means removing (zeroing out) some of these weights—typically those that are least important—so the model becomes:\n",
    "- Smaller in memory and disk size\n",
    "- Faster in inference (especially on sparse-optimized hardware)\n",
    "- Cheaper to deploy in production\n",
    "\n",
    "However, naive pruning methods often hurt performance. **SparseGPT** is designed to prune LLMs intelligently, keeping performance loss minimal even at very high sparsity levels (e.g., 50–60%).\n",
    "\n",
    "---\n",
    "\n",
    "##  What is SparseGPT?\n",
    "\n",
    "**SparseGPT** [[Frantar et al., 2023]](https://arxiv.org/abs/2301.00774) is a one-shot, data-aware pruning algorithm designed for transformer models at scale.  \n",
    "It operates as follows:\n",
    "- For each linear (fully connected) layer, SparseGPT analyzes the **weights** *and* the **layer’s real activation patterns** (collected on representative calibration data).\n",
    "- By solving a fast optimization problem for each layer, SparseGPT determines which weights can be set to zero with minimal loss to the model’s forward pass accuracy.\n",
    "- No retraining or fine-tuning is required after pruning: the pruned model can be used immediately.\n",
    "\n",
    "> **Reference:**  \n",
    "> Peter Frantar, Georgi Gerganov, Gary M. Satat, Dan Alistarh.  \n",
    "> [SparseGPT: Massive Language Models Can Be Accurately Pruned in One Shot](https://arxiv.org/abs/2301.00774)  \n",
    "> _arXiv preprint arXiv:2301.00774, 2023._  \n",
    "> [Official code on GitHub](https://github.com/IST-DASLab/SparseGPT)\n",
    "\n",
    "---\n",
    "\n",
    "##  **Step-by-Step Workflow in This Notebook**\n",
    "\n",
    "1. **Model Loading:**  \n",
    "   We load a pretrained HuggingFace LLM (e.g., TinyLlama) and move it to GPU for efficient computation.\n",
    "\n",
    "2. **Calibration Data Preparation:**  \n",
    "   We randomly sample a small number of real texts (e.g., from the AG News dataset), pad/truncate them to a fixed sequence length (e.g., 64 tokens), and use these as representative input for pruning.  \n",
    "   This step ensures that the pruned model keeps the weights most important for real-world data.\n",
    "\n",
    "3. **Layer-wise Activation Collection:**  \n",
    "   For every linear layer in the transformer, we attach a *forward hook* that records the input and output activations of that layer as the model processes the calibration data.  \n",
    "   This gives SparseGPT the context needed to make smart pruning decisions.\n",
    "\n",
    "4. **Layer Pruning Using SparseGPT:**  \n",
    "   For each linear layer:\n",
    "   - A `SparseGPT` object is created for that layer.\n",
    "   - The collected activations (input and output pairs) are fed into the pruner using `add_batch`.\n",
    "   - The actual pruning is performed with `fasterprune(sparsity=0.5)`, zeroing 50% of the weights based on the optimization described in the paper.\n",
    "\n",
    "5. **Model Evaluation:**  \n",
    "   After pruning, we evaluate the compressed model on a held-out test set.  \n",
    "   We compare the pruned model’s predictions, accuracy, and qualitative outputs with the original, unpruned model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Is This Important?**\n",
    "\n",
    "- **SparseGPT is widely recognized as the standard for LLM pruning**—it is used in many top-tier research papers and industry model releases.\n",
    "- The method is *data-aware*: the pruning is not random, but tailored to keep the parts of the model that are most important for real-world use cases.\n",
    "- **No retraining or fine-tuning is needed:**  \n",
    "  This means the whole compression process can be run in a few minutes on Colab, making it practical for research, prototyping, and even production.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1asUzDlFFWN5",
    "outputId": "da76475b-b9b8-4fce-bc9a-1e125523c2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer: model.layers.0.self_attn.q_proj\n",
      "time 1.19\n",
      "error 0.07008744776248932\n",
      "Pruned layer model.layers.0.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.0.self_attn.k_proj\n",
      "time 0.73\n",
      "error 0.018224719911813736\n",
      "Pruned layer model.layers.0.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.0.self_attn.v_proj\n",
      "time 0.51\n",
      "error 0.0229062270373106\n",
      "Pruned layer model.layers.0.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.0.self_attn.o_proj\n",
      "time 0.53\n",
      "error 0.004215043969452381\n",
      "Pruned layer model.layers.0.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.0.mlp.gate_proj\n",
      "time 0.52\n",
      "error 9.610349655151367\n",
      "Pruned layer model.layers.0.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.0.mlp.up_proj\n",
      "time 0.53\n",
      "error 10.317127227783203\n",
      "Pruned layer model.layers.0.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.0.mlp.down_proj\n",
      "time 1.80\n",
      "error 0.01179075799882412\n",
      "Pruned layer model.layers.0.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.1.self_attn.q_proj\n",
      "time 0.61\n",
      "error 5.261445999145508\n",
      "Pruned layer model.layers.1.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.1.self_attn.k_proj\n",
      "time 0.52\n",
      "error 1.3380651473999023\n",
      "Pruned layer model.layers.1.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.1.self_attn.v_proj\n",
      "time 0.52\n",
      "error 0.2385351061820984\n",
      "Pruned layer model.layers.1.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.1.self_attn.o_proj\n",
      "time 0.54\n",
      "error 0.07054997235536575\n",
      "Pruned layer model.layers.1.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.1.mlp.gate_proj\n",
      "time 0.51\n",
      "error 46.35109329223633\n",
      "Pruned layer model.layers.1.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.1.mlp.up_proj\n",
      "time 0.75\n",
      "error 41.63323974609375\n",
      "Pruned layer model.layers.1.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.1.mlp.down_proj\n",
      "time 1.62\n",
      "error 0.06695530563592911\n",
      "Pruned layer model.layers.1.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.2.self_attn.q_proj\n",
      "time 0.54\n",
      "error 22.93299102783203\n",
      "Pruned layer model.layers.2.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.2.self_attn.k_proj\n",
      "time 0.52\n",
      "error 6.73356819152832\n",
      "Pruned layer model.layers.2.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.2.self_attn.v_proj\n",
      "time 0.52\n",
      "error 0.480549156665802\n",
      "Pruned layer model.layers.2.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.2.self_attn.o_proj\n",
      "time 0.52\n",
      "error 0.1408688724040985\n",
      "Pruned layer model.layers.2.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.2.mlp.gate_proj\n",
      "time 0.81\n",
      "error 93.37704467773438\n",
      "Pruned layer model.layers.2.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.2.mlp.up_proj\n",
      "time 0.52\n",
      "error 84.79335021972656\n",
      "Pruned layer model.layers.2.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.2.mlp.down_proj\n",
      "time 1.62\n",
      "error 12.853803634643555\n",
      "Pruned layer model.layers.2.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.3.self_attn.q_proj\n",
      "time 0.53\n",
      "error 121.46157836914062\n",
      "Pruned layer model.layers.3.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.3.self_attn.k_proj\n",
      "time 0.54\n",
      "error 43.92652893066406\n",
      "Pruned layer model.layers.3.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.3.self_attn.v_proj\n",
      "time 0.69\n",
      "error 3.3213632106781006\n",
      "Pruned layer model.layers.3.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.3.self_attn.o_proj\n",
      "time 0.53\n",
      "error 0.07438316196203232\n",
      "Pruned layer model.layers.3.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.3.mlp.gate_proj\n",
      "time 0.54\n",
      "error 131.56716918945312\n",
      "Pruned layer model.layers.3.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.3.mlp.up_proj\n",
      "time 0.55\n",
      "error 113.98065185546875\n",
      "Pruned layer model.layers.3.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.3.mlp.down_proj\n",
      "time 1.60\n",
      "error 0.25275272130966187\n",
      "Pruned layer model.layers.3.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.4.self_attn.q_proj\n",
      "time 0.59\n",
      "error 266.9440612792969\n",
      "Pruned layer model.layers.4.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.4.self_attn.k_proj\n",
      "time 0.75\n",
      "error 107.78349304199219\n",
      "Pruned layer model.layers.4.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.4.self_attn.v_proj\n",
      "time 0.53\n",
      "error 5.782254219055176\n",
      "Pruned layer model.layers.4.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.4.self_attn.o_proj\n",
      "time 0.53\n",
      "error 0.15769422054290771\n",
      "Pruned layer model.layers.4.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.4.mlp.gate_proj\n",
      "time 0.54\n",
      "error 180.4989471435547\n",
      "Pruned layer model.layers.4.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.4.mlp.up_proj\n",
      "time 0.53\n",
      "error 151.58114624023438\n",
      "Pruned layer model.layers.4.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.4.mlp.down_proj\n",
      "time 2.05\n",
      "error 0.42426255345344543\n",
      "Pruned layer model.layers.4.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.5.self_attn.q_proj\n",
      "time 0.54\n",
      "error 228.96905517578125\n",
      "Pruned layer model.layers.5.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.5.self_attn.k_proj\n",
      "time 0.54\n",
      "error 90.46603393554688\n",
      "Pruned layer model.layers.5.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.5.self_attn.v_proj\n",
      "time 0.54\n",
      "error 6.2072529792785645\n",
      "Pruned layer model.layers.5.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.5.self_attn.o_proj\n",
      "time 0.54\n",
      "error 0.19090837240219116\n",
      "Pruned layer model.layers.5.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.5.mlp.gate_proj\n",
      "time 0.54\n",
      "error 221.2633056640625\n",
      "Pruned layer model.layers.5.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.5.mlp.up_proj\n",
      "time 0.71\n",
      "error 182.97921752929688\n",
      "Pruned layer model.layers.5.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.5.mlp.down_proj\n",
      "time 1.60\n",
      "error 0.604876697063446\n",
      "Pruned layer model.layers.5.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.6.self_attn.q_proj\n",
      "time 0.55\n",
      "error 176.6605682373047\n",
      "Pruned layer model.layers.6.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.6.self_attn.k_proj\n",
      "time 0.54\n",
      "error 69.79265594482422\n",
      "Pruned layer model.layers.6.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.6.self_attn.v_proj\n",
      "time 0.55\n",
      "error 4.902016639709473\n",
      "Pruned layer model.layers.6.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.6.self_attn.o_proj\n",
      "time 0.72\n",
      "error 0.29091113805770874\n",
      "Pruned layer model.layers.6.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.6.mlp.gate_proj\n",
      "time 0.53\n",
      "error 256.32342529296875\n",
      "Pruned layer model.layers.6.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.6.mlp.up_proj\n",
      "time 0.52\n",
      "error 200.93580627441406\n",
      "Pruned layer model.layers.6.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.6.mlp.down_proj\n",
      "time 1.60\n",
      "error 0.8259919881820679\n",
      "Pruned layer model.layers.6.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.7.self_attn.q_proj\n",
      "time 0.52\n",
      "error 237.3013916015625\n",
      "Pruned layer model.layers.7.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.7.self_attn.k_proj\n",
      "time 0.63\n",
      "error 82.07476806640625\n",
      "Pruned layer model.layers.7.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.7.self_attn.v_proj\n",
      "time 0.66\n",
      "error 8.824203491210938\n",
      "Pruned layer model.layers.7.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.7.self_attn.o_proj\n",
      "time 0.52\n",
      "error 0.5194612741470337\n",
      "Pruned layer model.layers.7.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.7.mlp.gate_proj\n",
      "time 0.52\n",
      "error 315.43096923828125\n",
      "Pruned layer model.layers.7.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.7.mlp.up_proj\n",
      "time 0.53\n",
      "error 213.73316955566406\n",
      "Pruned layer model.layers.7.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.7.mlp.down_proj\n",
      "time 1.60\n",
      "error 1.1187230348587036\n",
      "Pruned layer model.layers.7.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.8.self_attn.q_proj\n",
      "time 0.69\n",
      "error 250.42782592773438\n",
      "Pruned layer model.layers.8.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.8.self_attn.k_proj\n",
      "time 0.53\n",
      "error 93.29829406738281\n",
      "Pruned layer model.layers.8.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.8.self_attn.v_proj\n",
      "time 0.53\n",
      "error 7.988296031951904\n",
      "Pruned layer model.layers.8.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.8.self_attn.o_proj\n",
      "time 0.56\n",
      "error 0.3676019310951233\n",
      "Pruned layer model.layers.8.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.8.mlp.gate_proj\n",
      "time 0.53\n",
      "error 361.48907470703125\n",
      "Pruned layer model.layers.8.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.8.mlp.up_proj\n",
      "time 0.54\n",
      "error 269.4010009765625\n",
      "Pruned layer model.layers.8.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.8.mlp.down_proj\n",
      "time 1.90\n",
      "error 1.4089926481246948\n",
      "Pruned layer model.layers.8.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.9.self_attn.q_proj\n",
      "time 0.52\n",
      "error 279.9988708496094\n",
      "Pruned layer model.layers.9.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.9.self_attn.k_proj\n",
      "time 0.52\n",
      "error 112.33828735351562\n",
      "Pruned layer model.layers.9.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.9.self_attn.v_proj\n",
      "time 0.54\n",
      "error 8.663738250732422\n",
      "Pruned layer model.layers.9.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.9.self_attn.o_proj\n",
      "time 0.53\n",
      "error 0.7387456893920898\n",
      "Pruned layer model.layers.9.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.9.mlp.gate_proj\n",
      "time 0.73\n",
      "error 403.4617614746094\n",
      "Pruned layer model.layers.9.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.9.mlp.up_proj\n",
      "time 0.54\n",
      "error 284.5022277832031\n",
      "Pruned layer model.layers.9.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.9.mlp.down_proj\n",
      "time 1.58\n",
      "error 1.961906909942627\n",
      "Pruned layer model.layers.9.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.10.self_attn.q_proj\n",
      "time 0.53\n",
      "error 312.76727294921875\n",
      "Pruned layer model.layers.10.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.10.self_attn.k_proj\n",
      "time 0.53\n",
      "error 133.61795043945312\n",
      "Pruned layer model.layers.10.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.10.self_attn.v_proj\n",
      "time 0.53\n",
      "error 9.805932998657227\n",
      "Pruned layer model.layers.10.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.10.self_attn.o_proj\n",
      "time 0.82\n",
      "error 1.07599675655365\n",
      "Pruned layer model.layers.10.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.10.mlp.gate_proj\n",
      "time 0.54\n",
      "error 410.3204345703125\n",
      "Pruned layer model.layers.10.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.10.mlp.up_proj\n",
      "time 0.54\n",
      "error 307.7960510253906\n",
      "Pruned layer model.layers.10.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.10.mlp.down_proj\n",
      "time 1.60\n",
      "error 2.2244105339050293\n",
      "Pruned layer model.layers.10.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.11.self_attn.q_proj\n",
      "time 0.55\n",
      "error 339.2080078125\n",
      "Pruned layer model.layers.11.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.11.self_attn.k_proj\n",
      "time 0.70\n",
      "error 138.34156799316406\n",
      "Pruned layer model.layers.11.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.11.self_attn.v_proj\n",
      "time 0.53\n",
      "error 11.512554168701172\n",
      "Pruned layer model.layers.11.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.11.self_attn.o_proj\n",
      "time 0.52\n",
      "error 1.1744940280914307\n",
      "Pruned layer model.layers.11.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.11.mlp.gate_proj\n",
      "time 0.53\n",
      "error 441.639892578125\n",
      "Pruned layer model.layers.11.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.11.mlp.up_proj\n",
      "time 0.53\n",
      "error 330.2776184082031\n",
      "Pruned layer model.layers.11.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.11.mlp.down_proj\n",
      "time 1.82\n",
      "error 2.585184097290039\n",
      "Pruned layer model.layers.11.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.12.self_attn.q_proj\n",
      "time 0.52\n",
      "error 309.53570556640625\n",
      "Pruned layer model.layers.12.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.12.self_attn.k_proj\n",
      "time 0.53\n",
      "error 122.83857727050781\n",
      "Pruned layer model.layers.12.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.12.self_attn.v_proj\n",
      "time 0.53\n",
      "error 14.002002716064453\n",
      "Pruned layer model.layers.12.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.12.self_attn.o_proj\n",
      "time 0.52\n",
      "error 1.6127302646636963\n",
      "Pruned layer model.layers.12.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.12.mlp.gate_proj\n",
      "time 0.53\n",
      "error 481.4969482421875\n",
      "Pruned layer model.layers.12.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.12.mlp.up_proj\n",
      "time 0.76\n",
      "error 338.3106689453125\n",
      "Pruned layer model.layers.12.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.12.mlp.down_proj\n",
      "time 1.58\n",
      "error 3.2994894981384277\n",
      "Pruned layer model.layers.12.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.13.self_attn.q_proj\n",
      "time 0.51\n",
      "error 343.242919921875\n",
      "Pruned layer model.layers.13.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.13.self_attn.k_proj\n",
      "time 0.53\n",
      "error 141.89865112304688\n",
      "Pruned layer model.layers.13.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.13.self_attn.v_proj\n",
      "time 0.51\n",
      "error 11.884145736694336\n",
      "Pruned layer model.layers.13.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.13.self_attn.o_proj\n",
      "time 0.53\n",
      "error 2.069009780883789\n",
      "Pruned layer model.layers.13.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.13.mlp.gate_proj\n",
      "time 0.81\n",
      "error 510.403076171875\n",
      "Pruned layer model.layers.13.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.13.mlp.up_proj\n",
      "time 0.54\n",
      "error 368.49700927734375\n",
      "Pruned layer model.layers.13.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.13.mlp.down_proj\n",
      "time 1.59\n",
      "error 4.159440994262695\n",
      "Pruned layer model.layers.13.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.14.self_attn.q_proj\n",
      "time 0.54\n",
      "error 291.68280029296875\n",
      "Pruned layer model.layers.14.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.14.self_attn.k_proj\n",
      "time 0.54\n",
      "error 124.4175033569336\n",
      "Pruned layer model.layers.14.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.14.self_attn.v_proj\n",
      "time 0.71\n",
      "error 12.119322776794434\n",
      "Pruned layer model.layers.14.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.14.self_attn.o_proj\n",
      "time 0.55\n",
      "error 3.271911382675171\n",
      "Pruned layer model.layers.14.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.14.mlp.gate_proj\n",
      "time 0.55\n",
      "error 537.1181030273438\n",
      "Pruned layer model.layers.14.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.14.mlp.up_proj\n",
      "time 0.55\n",
      "error 405.3829650878906\n",
      "Pruned layer model.layers.14.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.14.mlp.down_proj\n",
      "time 1.61\n",
      "error 5.057087421417236\n",
      "Pruned layer model.layers.14.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.15.self_attn.q_proj\n",
      "time 0.75\n",
      "error 346.9995422363281\n",
      "Pruned layer model.layers.15.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.15.self_attn.k_proj\n",
      "time 0.53\n",
      "error 128.417724609375\n",
      "Pruned layer model.layers.15.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.15.self_attn.v_proj\n",
      "time 0.53\n",
      "error 15.719316482543945\n",
      "Pruned layer model.layers.15.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.15.self_attn.o_proj\n",
      "time 0.54\n",
      "error 3.1994848251342773\n",
      "Pruned layer model.layers.15.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.15.mlp.gate_proj\n",
      "time 0.54\n",
      "error 612.0721435546875\n",
      "Pruned layer model.layers.15.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.15.mlp.up_proj\n",
      "time 0.53\n",
      "error 473.3826904296875\n",
      "Pruned layer model.layers.15.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.15.mlp.down_proj\n",
      "time 2.11\n",
      "error 7.823883056640625\n",
      "Pruned layer model.layers.15.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.16.self_attn.q_proj\n",
      "time 0.54\n",
      "error 376.077392578125\n",
      "Pruned layer model.layers.16.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.16.self_attn.k_proj\n",
      "time 0.55\n",
      "error 144.62171936035156\n",
      "Pruned layer model.layers.16.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.16.self_attn.v_proj\n",
      "time 0.55\n",
      "error 19.750484466552734\n",
      "Pruned layer model.layers.16.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.16.self_attn.o_proj\n",
      "time 0.54\n",
      "error 2.934683322906494\n",
      "Pruned layer model.layers.16.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.16.mlp.gate_proj\n",
      "time 0.67\n",
      "error 809.26123046875\n",
      "Pruned layer model.layers.16.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.16.mlp.up_proj\n",
      "time 0.64\n",
      "error 592.4857177734375\n",
      "Pruned layer model.layers.16.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.16.mlp.down_proj\n",
      "time 1.58\n",
      "error 12.53914737701416\n",
      "Pruned layer model.layers.16.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.17.self_attn.q_proj\n",
      "time 0.53\n",
      "error 368.3338623046875\n",
      "Pruned layer model.layers.17.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.17.self_attn.k_proj\n",
      "time 0.53\n",
      "error 139.61541748046875\n",
      "Pruned layer model.layers.17.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.17.self_attn.v_proj\n",
      "time 0.54\n",
      "error 33.318965911865234\n",
      "Pruned layer model.layers.17.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.17.self_attn.o_proj\n",
      "time 0.74\n",
      "error 5.603815078735352\n",
      "Pruned layer model.layers.17.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.17.mlp.gate_proj\n",
      "time 0.54\n",
      "error 994.1831665039062\n",
      "Pruned layer model.layers.17.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.17.mlp.up_proj\n",
      "time 0.53\n",
      "error 738.385009765625\n",
      "Pruned layer model.layers.17.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.17.mlp.down_proj\n",
      "time 1.60\n",
      "error 17.716306686401367\n",
      "Pruned layer model.layers.17.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.18.self_attn.q_proj\n",
      "time 0.54\n",
      "error 416.1180114746094\n",
      "Pruned layer model.layers.18.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.18.self_attn.k_proj\n",
      "time 0.74\n",
      "error 140.39852905273438\n",
      "Pruned layer model.layers.18.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.18.self_attn.v_proj\n",
      "time 0.54\n",
      "error 37.015052795410156\n",
      "Pruned layer model.layers.18.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.18.self_attn.o_proj\n",
      "time 0.54\n",
      "error 5.682806968688965\n",
      "Pruned layer model.layers.18.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.18.mlp.gate_proj\n",
      "time 0.54\n",
      "error 1239.8001708984375\n",
      "Pruned layer model.layers.18.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.18.mlp.up_proj\n",
      "time 0.52\n",
      "error 942.4098510742188\n",
      "Pruned layer model.layers.18.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.18.mlp.down_proj\n",
      "time 1.66\n",
      "error 28.2073974609375\n",
      "Pruned layer model.layers.18.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.19.self_attn.q_proj\n",
      "time 0.72\n",
      "error 390.2656555175781\n",
      "Pruned layer model.layers.19.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.19.self_attn.k_proj\n",
      "time 0.55\n",
      "error 133.93162536621094\n",
      "Pruned layer model.layers.19.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.19.self_attn.v_proj\n",
      "time 0.52\n",
      "error 49.31559753417969\n",
      "Pruned layer model.layers.19.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.19.self_attn.o_proj\n",
      "time 0.52\n",
      "error 8.579202651977539\n",
      "Pruned layer model.layers.19.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.19.mlp.gate_proj\n",
      "time 0.52\n",
      "error 1470.677490234375\n",
      "Pruned layer model.layers.19.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.19.mlp.up_proj\n",
      "time 0.66\n",
      "error 1153.6298828125\n",
      "Pruned layer model.layers.19.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.19.mlp.down_proj\n",
      "time 1.58\n",
      "error 44.825843811035156\n",
      "Pruned layer model.layers.19.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.20.self_attn.q_proj\n",
      "time 0.52\n",
      "error 431.8624267578125\n",
      "Pruned layer model.layers.20.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.20.self_attn.k_proj\n",
      "time 0.53\n",
      "error 153.01211547851562\n",
      "Pruned layer model.layers.20.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.20.self_attn.v_proj\n",
      "time 0.52\n",
      "error 58.13263702392578\n",
      "Pruned layer model.layers.20.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.20.self_attn.o_proj\n",
      "time 0.53\n",
      "error 11.166828155517578\n",
      "Pruned layer model.layers.20.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.20.mlp.gate_proj\n",
      "time 0.69\n",
      "error 1689.4962158203125\n",
      "Pruned layer model.layers.20.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.20.mlp.up_proj\n",
      "time 0.52\n",
      "error 1372.371826171875\n",
      "Pruned layer model.layers.20.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.20.mlp.down_proj\n",
      "time 1.58\n",
      "error 71.1331558227539\n",
      "Pruned layer model.layers.20.mlp.down_proj successfully.\n",
      "Pruning layer: model.layers.21.self_attn.q_proj\n",
      "time 0.52\n",
      "error 464.18280029296875\n",
      "Pruned layer model.layers.21.self_attn.q_proj successfully.\n",
      "Pruning layer: model.layers.21.self_attn.k_proj\n",
      "time 0.53\n",
      "error 173.768798828125\n",
      "Pruned layer model.layers.21.self_attn.k_proj successfully.\n",
      "Pruning layer: model.layers.21.self_attn.v_proj\n",
      "time 0.76\n",
      "error 82.58203125\n",
      "Pruned layer model.layers.21.self_attn.v_proj successfully.\n",
      "Pruning layer: model.layers.21.self_attn.o_proj\n",
      "time 0.51\n",
      "error 24.777759552001953\n",
      "Pruned layer model.layers.21.self_attn.o_proj successfully.\n",
      "Pruning layer: model.layers.21.mlp.gate_proj\n",
      "time 0.51\n",
      "error 2040.97216796875\n",
      "Pruned layer model.layers.21.mlp.gate_proj successfully.\n",
      "Pruning layer: model.layers.21.mlp.up_proj\n",
      "time 0.53\n",
      "error 1397.72705078125\n",
      "Pruned layer model.layers.21.mlp.up_proj successfully.\n",
      "Pruning layer: model.layers.21.mlp.down_proj\n",
      "time 1.59\n",
      "error 107.24211883544922\n",
      "Pruned layer model.layers.21.mlp.down_proj successfully.\n",
      "Pruning layer: lm_head\n",
      "time 0.85\n",
      "error 137994.3125\n",
      "Pruned layer lm_head successfully.\n"
     ]
    }
   ],
   "source": [
    "for name, layer in linear_layers:\n",
    "    print(f\"Pruning layer: {name}\")\n",
    "\n",
    "    gpt = SparseGPT(layer)\n",
    "    activations = []\n",
    "\n",
    "    def hook_fn(module, inp, out):\n",
    "        # Save both input and output for each forward call\n",
    "        if inp[0].shape[1] == calib_seq_len:\n",
    "            # Save as a tuple (input, output)\n",
    "            activations.append((inp[0].detach().cpu(), out.detach().cpu()))\n",
    "\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in calib_data:\n",
    "            _ = model(batch)\n",
    "\n",
    "    handle.remove()\n",
    "\n",
    "    if len(activations) == 0:\n",
    "        print(f\"No activations captured for layer {name}\")\n",
    "        continue\n",
    "\n",
    "    # Now feed input/output pairs to add_batch\n",
    "    for inp, out in activations:\n",
    "        gpt.add_batch(inp.to(device), out.to(device))\n",
    "\n",
    "    gpt.fasterprune(sparsity=0.5)\n",
    "    print(f\"Pruned layer {name} successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIWm71FhI5UF",
    "outputId": "af6d0613-caa2-40d5-ce37-d65c4e3e7a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['question', 'answer'], dtype='object')\n",
      "                                            question  answer\n",
      "0  Wall St. Bears Claw Back Into the Black (Reute...       2\n",
      "1  Carlyle Looks Toward Commercial Aerospace (Reu...       2\n",
      "2  Oil and Economy Cloud Stocks' Outlook (Reuters...       2\n",
      "\n",
      "--- Sample results ---\n",
      "Input   : Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "Expected: 2\n",
      "Model   : Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again. The Dow Jones (DJD) was up 0.1 percent, while the S&P (SPY\n",
      "\n",
      "Input   : Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "Expected: 2\n",
      "Model   : Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "Silver Dials: The U.S. Defense Market: The U.S. Defense Market\n",
      "\n",
      "Input   : Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "Expected: 2\n",
      "Model   : Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "Silver, the lightest of the metals, is expected to\\hold steady, while gold is expected to\n",
      "\n",
      "Input   : Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "Expected: 2\n",
      "Model   : Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "Middle East: Iran, Iraq to Sign Agreement on Oil Exploration (Reuters)\n",
      "\n",
      "Input   : Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
      "Expected: 2\n",
      "Model   : Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
      "ICC 2000: The World Cup in the 21st century\n",
      "The World Cup in\n",
      "\n",
      "Exact match accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load your evaluation CSV\n",
    "eval_df = pd.read_csv(\"/content/eval_agnews.csv\")\n",
    "\n",
    "# 2. Show the columns and first few rows to understand the structure\n",
    "print(\"Columns:\", eval_df.columns)\n",
    "print(eval_df.head(3))\n",
    "\n",
    "# 3. Pick the input and answer columns automatically (or set manually if you want)\n",
    "input_col, answer_col = eval_df.columns[:2]  # assumes first is input/question, second is answer/label\n",
    "\n",
    "# (OPTIONAL) Uncomment below and set manually if you want:\n",
    "# input_col = \"question\"\n",
    "# answer_col = \"answer\"\n",
    "\n",
    "eval_inputs = eval_df[input_col].astype(str).tolist()\n",
    "eval_answers = eval_df[answer_col].astype(str).tolist()\n",
    "\n",
    "# 4. Run the pruned model on your inputs\n",
    "device = next(model.parameters()).device\n",
    "pruned_outputs = []\n",
    "for inp in eval_inputs:\n",
    "    inputs = tokenizer(inp, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=24)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    pruned_outputs.append(decoded)\n",
    "\n",
    "# 5. Print a few qualitative comparisons\n",
    "print(\"\\n--- Sample results ---\")\n",
    "for i in range(min(5, len(eval_inputs))):\n",
    "    print(f\"Input   : {eval_inputs[i]}\")\n",
    "    print(f\"Expected: {eval_answers[i]}\")\n",
    "    print(f\"Model   : {pruned_outputs[i]}\\n\")\n",
    "\n",
    "# 6. (Optional) Compute simple exact match accuracy for classification tasks\n",
    "def match_answer(pred, true):\n",
    "    return pred.strip().lower() == true.strip().lower()\n",
    "\n",
    "accuracy = sum(match_answer(p, t) for p, t in zip(pruned_outputs, eval_answers)) / len(eval_answers)\n",
    "print(f\"Exact match accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_PR0pWNMJPw",
    "outputId": "1982cad8-17dd-4da1-afb6-a47df14720ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "base_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j77xvglGNOJz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_label_after_label_colon(output):\n",
    "    match = re.search(r\"Label:\\s*([0-3])\", output)\n",
    "    return match.group(1) if match else \"-1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKzM3JbGN1k9"
   },
   "outputs": [],
   "source": [
    "device = next(model.parameters()).device\n",
    "predicted_labels = []\n",
    "\n",
    "import re\n",
    "\n",
    "for article in eval_questions:\n",
    "    prompt = few_shot_prompt.format(article=article)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=8)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the last number after \"Label:\" in the output (most robust)\n",
    "    matches = list(re.finditer(r\"Label:\\s*([0-3])\", decoded))\n",
    "    predicted_label = matches[-1].group(1) if matches else \"-1\"\n",
    "    predicted_labels.append(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuydCz2OQQpC",
    "outputId": "532e67ef-89f8-48d9-f513-93af1b78c32c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  \\\n",
      "0  Jarryd Hayne's move to the NFL is a boost for ...   \n",
      "1  An anorexic teenager whose weight dropped to j...   \n",
      "2  (CNN)For years, they've wanted six seasons and...   \n",
      "\n",
      "                                             summary  \n",
      "0  Jarryd Hayne quit the NRL in October to try an...  \n",
      "1  Faith March's dropped to just five stone as sh...  \n",
      "2  The fan favorite comedy \"Community\" returns fo...  \n",
      "Number of samples: 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your uploaded sample\n",
    "df = pd.read_csv(\"/content/cnn_dailymail_sample.csv\")\n",
    "print(df.head(3))\n",
    "print(f\"Number of samples: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BYxbtLSUOh2",
    "outputId": "93898771-8f29-4487-8541-5524f25a24c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['document', 'summary'], dtype='object')\n",
      "                                            document  \\\n",
      "0  Jarryd Hayne's move to the NFL is a boost for ...   \n",
      "1  An anorexic teenager whose weight dropped to j...   \n",
      "2  (CNN)For years, they've wanted six seasons and...   \n",
      "\n",
      "                                             summary  \n",
      "0  Jarryd Hayne quit the NRL in October to try an...  \n",
      "1  Faith March's dropped to just five stone as sh...  \n",
      "2  The fan favorite comedy \"Community\" returns fo...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/cnn_dailymail_sample.csv\")\n",
    "\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSZgYQxRUgX0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZDsFFzhRT6H3",
    "outputId": "b9b03072-dcab-4fb4-b584-56ec0dd5f078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article: Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try h ...\n",
      "Reference Summary: Jarryd Hayne quit the NRL in October to try and get into American Football .\n",
      "This week, he signed a three-year contract with the San Francisco 49ers .\n",
      "The chairman of the US Association of Rugby League welcomed his arrival .\n",
      "Model Summary: Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, according to Peter Illfield, chairman of the US Association of Rugby League. Hayne, who played rugby league for Australia, has signed a three-year contract with the San Francisco 49\n",
      "---\n",
      "Article: An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other t ...\n",
      "Reference Summary: Faith March's dropped to just five stone as she suffered from anorexia .\n",
      "The 18-year-old from Essex was living on just coffee and no food .\n",
      "After she collapsed in the bathroom, she had hospital treatment .\n",
      "Has now launched a patisserie business to help her recover .\n",
      "Model Summary: Summarize the following article:\n",
      "\n",
      "An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other than coffee when she dropped to her lowest weight in March of last year. After several ill-fated attempts to fight the illness, Faith collapsed in her bathroom where she was found by her boyfriend - and her family told her they feared for her life if she didn't get help. Scroll down for video . Faith March's weight dropped to just five stone when she was suffering from anorexia (left) but she is now in recovery and has set up her own patisserie business (right) After treatment at the Priory Hospital in Chelmsford, Faith is now at a healthier weight and credits the starting of her patisserie business, Whisk of Faith, as kick-starting her recovery. Faith said: 'This business has helped me get out of a massive hole. If I'm honest, it was a hole I never thought I would get out of. It just seemed like a never-ending cycle of problems.' Faith says she has had problems with her stomach which have baffled doctors for most of her life, but at 14 she was finally told her colon had collapsed and had surgery to insert a stent into her stomach. That returned her to some sort of normality, but in March last year her eating problems began to worsen and those around her began to notice. Faith said: 'My boyfriend, mum and dad noticed I had stopped eating again, I was just surviving on coffee,' she said. 'I was doing a cookery course at the time as well as working part time at a pub so I was surrounded by food but I just couldn't bring myself to eat anything. Faith looks happy as she poses with some of her cooking equipment and her freshly baked goods . Faith looks happy as she poses in her chefs outfit complete with an apron with the name of her company on the front . 'My boss told me to take some time out because it was clear I wasn't 100 per cent so I went to the doctors and that is when I was diagnosed with anorexia. 'I was in a really bad place and I didn't know what to do. I was so scared and I didn't want to be seen as weak. I was so scared of what people would think of me. 'I was so scared\n",
      "---\n",
      "Article: (CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even  ...\n",
      "Reference Summary: The fan favorite comedy \"Community\" returns for a sixth season on Yahoo .\n",
      "The series is just as weirdly hilarious as ever, with surprises in store .\n",
      "Critics and fans loved the premiere .\n",
      "Model Summary: Summarize the following article:\n",
      "\n",
      "(CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even cancellation by NBC could kill the fan favorite sitcom, as Yahoo streamed the first two episodes of the new season early Tuesday. The show appears to have suffered no decline in quality in the move to Yahoo, though cast member Yvette Nicole Brown (now on CBS' \"The Odd Couple\") had to leave the show for family reasons. The premiere briefly touches on this in its own way, with Shirley's absence sounding a lot like the setup for a spinoff (as the character Abed points out, along with several other inconsistencies throughout the episode). The \"new Shirley,\" as Dean joked, is Paget Brewster's Frankie, who exists to play the disapproving authority figure to antagonize the former study group. Soon, the group was running a Prohibition-esque bar before they could learn to live with Frankie. And how about the end of the first episode, which gave us a look at the spinoff, \"The Butcher and the Baker,\" with Shirley and Steven Weber as a Southern lawyer? Episode two got even more out there, with Dean Pelton's adventures in 1990s-style virtual reality and a less-successful plotline involving Britta's parents. Most interesting was the introduction of Keith David as '90s tech genius Elroy. Critics praised the show, with The Hollywood Reporter's Amy Amatangelo saying, \"Everything fans loved about Community remains -- the first two episodes are chock-full of increasingly bizarre pop-culture references (Portuguese Gremlins, anyone?) and meta commentary. The show has seamlessly transferred to an online venue.\" Time's James Poniewozik also liked it (despite some reservations about the plot), writing, \"The first thing that matters is if the latest reboot still has the comedy goods, and it does.\" Soon after the show went online, devoted fans began to speculate about a movie. But NBC, which owns the show, has said it's not happening. \"We're not going to do a movie,\" NBC Entertainment Chairman Robert Greenblatt told The Hollywood Reporter. \"We're not going to do a movie. We'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "device = next(model.parameters()).device\n",
    "\n",
    "inputs = df[\"document\"].tolist()\n",
    "reference_summaries = df[\"summary\"].tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "for article in inputs:\n",
    "    prompt = f\"Summarize the following article:\\n\\n{article}\\n\\nSummary:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**input_ids, max_new_tokens=64)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if \"Summary:\" in summary:\n",
    "        summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    generated_summaries.append(summary)\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Article:\", inputs[i][:200], \"...\")\n",
    "    print(\"Reference Summary:\", reference_summaries[i])\n",
    "    print(\"Model Summary:\", generated_summaries[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDF-04ZNUhN-",
    "outputId": "90d8ebd9-de95-4121-e918-6b47dcec1c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "ROUGE-1 F1 avg: 0.224\n",
      "ROUGE-2 F1 avg: 0.103\n",
      "ROUGE-L F1 avg: 0.146\n",
      "ARTICLE: Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try h ...\n",
      "REFERENCE: Jarryd Hayne quit the NRL in October to try and get into American Football .\n",
      "This week, he signed a three-year contract with the San Francisco 49ers .\n",
      "The chairman of the US Association of Rugby League welcomed his arrival .\n",
      "GENERATED: Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, according to Peter Illfield, chairman of the US Association of Rugby League. Hayne, who played rugby league for Australia, has signed a three-year contract with the San Francisco 49\n",
      "---\n",
      "ARTICLE: An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other t ...\n",
      "REFERENCE: Faith March's dropped to just five stone as she suffered from anorexia .\n",
      "The 18-year-old from Essex was living on just coffee and no food .\n",
      "After she collapsed in the bathroom, she had hospital treatment .\n",
      "Has now launched a patisserie business to help her recover .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other than coffee when she dropped to her lowest weight in March of last year. After several ill-fated attempts to fight the illness, Faith collapsed in her bathroom where she was found by her boyfriend - and her family told her they feared for her life if she didn't get help. Scroll down for video . Faith March's weight dropped to just five stone when she was suffering from anorexia (left) but she is now in recovery and has set up her own patisserie business (right) After treatment at the Priory Hospital in Chelmsford, Faith is now at a healthier weight and credits the starting of her patisserie business, Whisk of Faith, as kick-starting her recovery. Faith said: 'This business has helped me get out of a massive hole. If I'm honest, it was a hole I never thought I would get out of. It just seemed like a never-ending cycle of problems.' Faith says she has had problems with her stomach which have baffled doctors for most of her life, but at 14 she was finally told her colon had collapsed and had surgery to insert a stent into her stomach. That returned her to some sort of normality, but in March last year her eating problems began to worsen and those around her began to notice. Faith said: 'My boyfriend, mum and dad noticed I had stopped eating again, I was just surviving on coffee,' she said. 'I was doing a cookery course at the time as well as working part time at a pub so I was surrounded by food but I just couldn't bring myself to eat anything. Faith looks happy as she poses with some of her cooking equipment and her freshly baked goods . Faith looks happy as she poses in her chefs outfit complete with an apron with the name of her company on the front . 'My boss told me to take some time out because it was clear I wasn't 100 per cent so I went to the doctors and that is when I was diagnosed with anorexia. 'I was in a really bad place and I didn't know what to do. I was so scared and I didn't want to be seen as weak. I was so scared of what people would think of me. 'I was so scared\n",
      "---\n",
      "ARTICLE: (CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even  ...\n",
      "REFERENCE: The fan favorite comedy \"Community\" returns for a sixth season on Yahoo .\n",
      "The series is just as weirdly hilarious as ever, with surprises in store .\n",
      "Critics and fans loved the premiere .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "(CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even cancellation by NBC could kill the fan favorite sitcom, as Yahoo streamed the first two episodes of the new season early Tuesday. The show appears to have suffered no decline in quality in the move to Yahoo, though cast member Yvette Nicole Brown (now on CBS' \"The Odd Couple\") had to leave the show for family reasons. The premiere briefly touches on this in its own way, with Shirley's absence sounding a lot like the setup for a spinoff (as the character Abed points out, along with several other inconsistencies throughout the episode). The \"new Shirley,\" as Dean joked, is Paget Brewster's Frankie, who exists to play the disapproving authority figure to antagonize the former study group. Soon, the group was running a Prohibition-esque bar before they could learn to live with Frankie. And how about the end of the first episode, which gave us a look at the spinoff, \"The Butcher and the Baker,\" with Shirley and Steven Weber as a Southern lawyer? Episode two got even more out there, with Dean Pelton's adventures in 1990s-style virtual reality and a less-successful plotline involving Britta's parents. Most interesting was the introduction of Keith David as '90s tech genius Elroy. Critics praised the show, with The Hollywood Reporter's Amy Amatangelo saying, \"Everything fans loved about Community remains -- the first two episodes are chock-full of increasingly bizarre pop-culture references (Portuguese Gremlins, anyone?) and meta commentary. The show has seamlessly transferred to an online venue.\" Time's James Poniewozik also liked it (despite some reservations about the plot), writing, \"The first thing that matters is if the latest reboot still has the comedy goods, and it does.\" Soon after the show went online, devoted fans began to speculate about a movie. But NBC, which owns the show, has said it's not happening. \"We're not going to do a movie,\" NBC Entertainment Chairman Robert Greenblatt told The Hollywood Reporter. \"We're not going to do a movie. We'\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "articles = df[\"document\"].astype(str).tolist()\n",
    "reference_summaries = df[\"summary\"].astype(str).tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "for article in articles:\n",
    "    prompt = f\"Summarize the following article:\\n\\n{article}\\n\\nSummary:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**input_ids, max_new_tokens=64)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if \"Summary:\" in summary:\n",
    "        summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    generated_summaries.append(summary)\n",
    "\n",
    "!pip install rouge-score --quiet\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = [scorer.score(ref, gen) for ref, gen in zip(reference_summaries, generated_summaries)]\n",
    "\n",
    "rouge1_avg = sum([s['rouge1'].fmeasure for s in scores]) / len(scores)\n",
    "rouge2_avg = sum([s['rouge2'].fmeasure for s in scores]) / len(scores)\n",
    "rougeL_avg = sum([s['rougeL'].fmeasure for s in scores]) / len(scores)\n",
    "\n",
    "print(f\"ROUGE-1 F1 avg: {rouge1_avg:.3f}\")\n",
    "print(f\"ROUGE-2 F1 avg: {rouge2_avg:.3f}\")\n",
    "print(f\"ROUGE-L F1 avg: {rougeL_avg:.3f}\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"ARTICLE:\", articles[i][:200], \"...\")\n",
    "    print(\"REFERENCE:\", reference_summaries[i])\n",
    "    print(\"GENERATED:\", generated_summaries[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwo_hIYtW2ma"
   },
   "outputs": [],
   "source": [
    "calib_texts = df[\"document\"].astype(str).tolist()[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSYYI7EDW8Qp",
    "outputId": "77a1eea2-0cb7-4f29-8329-c3b1cd4822c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRUNED] ROUGE-1 F1 avg: 0.258\n",
      "[PRUNED] ROUGE-2 F1 avg: 0.109\n",
      "[PRUNED] ROUGE-L F1 avg: 0.173\n",
      "ARTICLE: Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try h ...\n",
      "REFERENCE: Jarryd Hayne quit the NRL in October to try and get into American Football .\n",
      "This week, he signed a three-year contract with the San Francisco 49ers .\n",
      "The chairman of the US Association of Rugby League welcomed his arrival .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try his luck in American football and was this week given a three-year contract with the San Francisco 49ers. Peter Illfield, chairman of US Association of Rugby League, said: 'Jarryd, at 27, is one of the most gifted and talented rugby league players in Australia. He is an extraordinary athlete. Jarryd Hayne (right) has signed with the San Francisco 49ers after quitting the NRL in October . Hayne, who played rugby league for Australia, has signed a three year contract with the 49ers . 'His three-year deal with the 49ers, as an expected running back, gives the USA Rugby League a connection with the American football lover like never before. 'Jarryd's profile and playing ability will bring our sport to the attention of many. It also has the possibility of showing the American college athlete the possibilities of transition and adaptation for them to play rugby league in the USA. 'The 49ers are a great team and we are delighted to have Jarryd on board. He is\n",
      "---\n",
      "ARTICLE: An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other t ...\n",
      "REFERENCE: Faith March's dropped to just five stone as she suffered from anorexia .\n",
      "The 18-year-old from Essex was living on just coffee and no food .\n",
      "After she collapsed in the bathroom, she had hospital treatment .\n",
      "Has now launched a patisserie business to help her recover .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other than coffee when she dropped to her lowest weight in March of last year. After several ill-fated attempts to fight the illness, Faith collapsed in her bathroom where she was found by her boyfriend - and her family told her they feared for her life if she didn't get help. Scroll down for video . Faith March's weight dropped to just five stone when she was suffering from anorexia (left) but she is now in recovery and has set up her own patisserie business (right) After treatment at the Priory Hospital in Chelmsford, Faith is now at a healthier weight and credits the starting of her patisserie business, Whisk of Faith, as kick-starting her recovery. Faith said: 'This business has helped me get out of a massive hole. If I'm honest, it was a hole I never thought I would get out of. It's been a long road but I'm happy and healthy now. 'I've been in and out of hospital for a long time and I\n",
      "---\n",
      "ARTICLE: (CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even  ...\n",
      "REFERENCE: The fan favorite comedy \"Community\" returns for a sixth season on Yahoo .\n",
      "The series is just as weirdly hilarious as ever, with surprises in store .\n",
      "Critics and fans loved the premiere .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "(CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even cancellation by NBC could kill the fan favorite sitcom, as Yahoo streamed the first two episodes of the new season early Tuesday. The show appears to have suffered no decline in quality in the move to Yahoo, though cast member Yvette Nicole Brown (now on CBS' \"The Odd Couple\") had to leave the show for family reasons. The premiere briefly touches on this in its own way, with Shirley's absence sounding a lot like the setup for a spinoff (as the character Abed points out, along with several other inconsistencies throughout the episode). The \"new Shirley,\" as Dean joked, is Paget Brewster's Frankie, who exists to play the disapproving authority figure to antagonize the former study group. Soon, the group was running a restaurant, and the episode ends with the group's new restaurant, \"The Pain and Pleasure of Being Frankie,\" being featured in a local\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "articles = df[\"document\"].astype(str).tolist()[:3]\n",
    "reference_summaries = df[\"summary\"].astype(str).tolist()[:3]\n",
    "generated_summaries = []\n",
    "\n",
    "for article in articles:\n",
    "    prompt = f\"Summarize the following article:\\n\\n{article}\\n\\nSummary:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**input_ids, max_new_tokens=32)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if \"Summary:\" in summary:\n",
    "        summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    generated_summaries.append(summary)\n",
    "    del input_ids, output_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = [scorer.score(ref, gen) for ref, gen in zip(reference_summaries, generated_summaries)]\n",
    "\n",
    "rouge1_avg = sum([s['rouge1'].fmeasure for s in scores]) / len(scores)\n",
    "rouge2_avg = sum([s['rouge2'].fmeasure for s in scores]) / len(scores)\n",
    "rougeL_avg = sum([s['rougeL'].fmeasure for s in scores]) / len(scores)\n",
    "\n",
    "print(f\"[PRUNED] ROUGE-1 F1 avg: {rouge1_avg:.3f}\")\n",
    "print(f\"[PRUNED] ROUGE-2 F1 avg: {rouge2_avg:.3f}\")\n",
    "print(f\"[PRUNED] ROUGE-L F1 avg: {rougeL_avg:.3f}\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"ARTICLE:\", articles[i][:200], \"...\")\n",
    "    print(\"REFERENCE:\", reference_summaries[i])\n",
    "    print(\"GENERATED:\", generated_summaries[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADSWy7oUXSlF"
   },
   "outputs": [],
   "source": [
    "calib_texts = df[\"document\"].astype(str).tolist()[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CzG7-hCZXUOA"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "device = next(model.parameters()).device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyiX5mvdX8LR"
   },
   "outputs": [],
   "source": [
    "def get_activation_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        if name not in activation_dict:\n",
    "            activation_dict[name] = []\n",
    "        act = input[0].detach()\n",
    "        if act.ndim > 2:\n",
    "            act = act.view(-1, act.shape[-1])\n",
    "        activation_dict[name].append(act.to(device))\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRztycC1YLuo",
    "outputId": "f19851dd-db7d-4350-ac9e-4f756f406bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning layer model.layers.0.self_attn.q_proj ...\n",
      "time 1.94\n",
      "error 0.2700767517089844\n",
      "Pruning layer model.layers.0.self_attn.k_proj ...\n",
      "time 1.50\n",
      "error 0.07024785131216049\n",
      "Pruning layer model.layers.0.self_attn.v_proj ...\n",
      "time 0.74\n",
      "error 0.0885753184556961\n",
      "Pruning layer model.layers.0.self_attn.o_proj ...\n",
      "time 0.82\n",
      "error 0.012682058848440647\n",
      "Pruning layer model.layers.0.mlp.gate_proj ...\n",
      "time 1.18\n",
      "error 41.7613639831543\n",
      "Pruning layer model.layers.0.mlp.up_proj ...\n",
      "time 1.07\n",
      "error 44.91261291503906\n",
      "Pruning layer model.layers.0.mlp.down_proj ...\n",
      "time 2.58\n",
      "error 0.04819351062178612\n",
      "Pruning layer model.layers.1.self_attn.q_proj ...\n",
      "time 0.58\n",
      "error 21.750154495239258\n",
      "Pruning layer model.layers.1.self_attn.k_proj ...\n",
      "time 0.57\n",
      "error 5.511509895324707\n",
      "Pruning layer model.layers.1.self_attn.v_proj ...\n",
      "time 0.53\n",
      "error 0.9868661165237427\n",
      "Pruning layer model.layers.1.self_attn.o_proj ...\n",
      "time 0.56\n",
      "error 0.2096840888261795\n",
      "Pruning layer model.layers.1.mlp.gate_proj ...\n",
      "time 0.54\n",
      "error 197.0167999267578\n",
      "Pruning layer model.layers.1.mlp.up_proj ...\n",
      "time 0.55\n",
      "error 176.96804809570312\n",
      "Pruning layer model.layers.1.mlp.down_proj ...\n",
      "time 1.59\n",
      "error 0.27043771743774414\n",
      "Pruning layer model.layers.2.self_attn.q_proj ...\n",
      "time 0.60\n",
      "error 98.211669921875\n",
      "Pruning layer model.layers.2.self_attn.k_proj ...\n",
      "time 0.74\n",
      "error 28.835609436035156\n",
      "Pruning layer model.layers.2.self_attn.v_proj ...\n",
      "time 0.74\n",
      "error 2.0654709339141846\n",
      "Pruning layer model.layers.2.self_attn.o_proj ...\n",
      "time 1.07\n",
      "error 0.5419479608535767\n",
      "Pruning layer model.layers.2.mlp.gate_proj ...\n",
      "time 0.92\n",
      "error 385.2333068847656\n",
      "Pruning layer model.layers.2.mlp.up_proj ...\n",
      "time 0.94\n",
      "error 350.2975769042969\n",
      "Pruning layer model.layers.2.mlp.down_proj ...\n",
      "time 2.61\n",
      "error 14.32645034790039\n",
      "Pruning layer model.layers.3.self_attn.q_proj ...\n",
      "time 0.54\n",
      "error 519.9129028320312\n",
      "Pruning layer model.layers.3.self_attn.k_proj ...\n",
      "time 0.56\n",
      "error 186.86354064941406\n",
      "Pruning layer model.layers.3.self_attn.v_proj ...\n",
      "time 0.54\n",
      "error 14.367008209228516\n",
      "Pruning layer model.layers.3.self_attn.o_proj ...\n",
      "time 0.54\n",
      "error 0.29844993352890015\n",
      "Pruning layer model.layers.3.mlp.gate_proj ...\n",
      "time 0.54\n",
      "error 555.1934814453125\n",
      "Pruning layer model.layers.3.mlp.up_proj ...\n",
      "time 0.56\n",
      "error 481.7665710449219\n",
      "Pruning layer model.layers.3.mlp.down_proj ...\n",
      "time 1.74\n",
      "error 1.1421806812286377\n",
      "Pruning layer model.layers.4.self_attn.q_proj ...\n",
      "time 0.68\n",
      "error 1142.413818359375\n",
      "Pruning layer model.layers.4.self_attn.k_proj ...\n",
      "time 0.82\n",
      "error 458.2939453125\n",
      "Pruning layer model.layers.4.self_attn.v_proj ...\n",
      "time 0.57\n",
      "error 24.821876525878906\n",
      "Pruning layer model.layers.4.self_attn.o_proj ...\n",
      "time 0.54\n",
      "error 0.847148060798645\n",
      "Pruning layer model.layers.4.mlp.gate_proj ...\n",
      "time 0.57\n",
      "error 765.513671875\n",
      "Pruning layer model.layers.4.mlp.up_proj ...\n",
      "time 0.55\n",
      "error 644.429931640625\n",
      "Pruning layer model.layers.4.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 1.9674746990203857\n",
      "Pruning layer model.layers.5.self_attn.q_proj ...\n",
      "time 0.53\n",
      "error 984.1341552734375\n",
      "Pruning layer model.layers.5.self_attn.k_proj ...\n",
      "time 0.56\n",
      "error 386.59991455078125\n",
      "Pruning layer model.layers.5.self_attn.v_proj ...\n",
      "time 0.55\n",
      "error 26.80025291442871\n",
      "Pruning layer model.layers.5.self_attn.o_proj ...\n",
      "time 0.56\n",
      "error 0.9155019521713257\n",
      "Pruning layer model.layers.5.mlp.gate_proj ...\n",
      "time 0.56\n",
      "error 962.958251953125\n",
      "Pruning layer model.layers.5.mlp.up_proj ...\n",
      "time 0.53\n",
      "error 796.24169921875\n",
      "Pruning layer model.layers.5.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 3.0105745792388916\n",
      "Pruning layer model.layers.6.self_attn.q_proj ...\n",
      "time 0.53\n",
      "error 772.764404296875\n",
      "Pruning layer model.layers.6.self_attn.k_proj ...\n",
      "time 0.55\n",
      "error 303.8789978027344\n",
      "Pruning layer model.layers.6.self_attn.v_proj ...\n",
      "time 0.77\n",
      "error 21.577974319458008\n",
      "Pruning layer model.layers.6.self_attn.o_proj ...\n",
      "time 0.74\n",
      "error 1.4314234256744385\n",
      "Pruning layer model.layers.6.mlp.gate_proj ...\n",
      "time 0.76\n",
      "error 1121.326416015625\n",
      "Pruning layer model.layers.6.mlp.up_proj ...\n",
      "time 0.56\n",
      "error 883.2376708984375\n",
      "Pruning layer model.layers.6.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 4.209016799926758\n",
      "Pruning layer model.layers.7.self_attn.q_proj ...\n",
      "time 0.55\n",
      "error 1047.2696533203125\n",
      "Pruning layer model.layers.7.self_attn.k_proj ...\n",
      "time 0.57\n",
      "error 362.18310546875\n",
      "Pruning layer model.layers.7.self_attn.v_proj ...\n",
      "time 0.55\n",
      "error 39.42986297607422\n",
      "Pruning layer model.layers.7.self_attn.o_proj ...\n",
      "time 0.53\n",
      "error 2.4276227951049805\n",
      "Pruning layer model.layers.7.mlp.gate_proj ...\n",
      "time 0.56\n",
      "error 1355.696533203125\n",
      "Pruning layer model.layers.7.mlp.up_proj ...\n",
      "time 0.54\n",
      "error 922.6688232421875\n",
      "Pruning layer model.layers.7.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 5.429632663726807\n",
      "Pruning layer model.layers.8.self_attn.q_proj ...\n",
      "time 0.54\n",
      "error 1066.2568359375\n",
      "Pruning layer model.layers.8.self_attn.k_proj ...\n",
      "time 0.54\n",
      "error 396.0968933105469\n",
      "Pruning layer model.layers.8.self_attn.v_proj ...\n",
      "time 0.55\n",
      "error 34.59046173095703\n",
      "Pruning layer model.layers.8.self_attn.o_proj ...\n",
      "time 0.54\n",
      "error 2.362412452697754\n",
      "Pruning layer model.layers.8.mlp.gate_proj ...\n",
      "time 0.69\n",
      "error 1543.37841796875\n",
      "Pruning layer model.layers.8.mlp.up_proj ...\n",
      "time 0.71\n",
      "error 1155.208251953125\n",
      "Pruning layer model.layers.8.mlp.down_proj ...\n",
      "time 1.88\n",
      "error 7.16230583190918\n",
      "Pruning layer model.layers.9.self_attn.q_proj ...\n",
      "time 0.54\n",
      "error 1210.74365234375\n",
      "Pruning layer model.layers.9.self_attn.k_proj ...\n",
      "time 0.53\n",
      "error 491.88922119140625\n",
      "Pruning layer model.layers.9.self_attn.v_proj ...\n",
      "time 0.54\n",
      "error 37.87254333496094\n",
      "Pruning layer model.layers.9.self_attn.o_proj ...\n",
      "time 0.56\n",
      "error 4.75211238861084\n",
      "Pruning layer model.layers.9.mlp.gate_proj ...\n",
      "time 0.54\n",
      "error 1728.67919921875\n",
      "Pruning layer model.layers.9.mlp.up_proj ...\n",
      "time 0.56\n",
      "error 1222.763916015625\n",
      "Pruning layer model.layers.9.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 10.260018348693848\n",
      "Pruning layer model.layers.10.self_attn.q_proj ...\n",
      "time 0.54\n",
      "error 1365.3492431640625\n",
      "Pruning layer model.layers.10.self_attn.k_proj ...\n",
      "time 0.54\n",
      "error 580.836181640625\n",
      "Pruning layer model.layers.10.self_attn.v_proj ...\n",
      "time 0.54\n",
      "error 42.72085189819336\n",
      "Pruning layer model.layers.10.self_attn.o_proj ...\n",
      "time 0.53\n",
      "error 6.20640754699707\n",
      "Pruning layer model.layers.10.mlp.gate_proj ...\n",
      "time 0.56\n",
      "error 1800.7337646484375\n",
      "Pruning layer model.layers.10.mlp.up_proj ...\n",
      "time 0.54\n",
      "error 1349.252685546875\n",
      "Pruning layer model.layers.10.mlp.down_proj ...\n",
      "time 1.99\n",
      "error 12.06826400756836\n",
      "Pruning layer model.layers.11.self_attn.q_proj ...\n",
      "time 0.78\n",
      "error 1517.95654296875\n",
      "Pruning layer model.layers.11.self_attn.k_proj ...\n",
      "time 0.55\n",
      "error 615.5794677734375\n",
      "Pruning layer model.layers.11.self_attn.v_proj ...\n",
      "time 0.53\n",
      "error 51.495391845703125\n",
      "Pruning layer model.layers.11.self_attn.o_proj ...\n",
      "time 0.55\n",
      "error 7.185544490814209\n",
      "Pruning layer model.layers.11.mlp.gate_proj ...\n",
      "time 0.55\n",
      "error 2007.0120849609375\n",
      "Pruning layer model.layers.11.mlp.up_proj ...\n",
      "time 0.56\n",
      "error 1507.6114501953125\n",
      "Pruning layer model.layers.11.mlp.down_proj ...\n",
      "time 1.60\n",
      "error 14.833650588989258\n",
      "Pruning layer model.layers.12.self_attn.q_proj ...\n",
      "time 0.59\n",
      "error 1422.1119384765625\n",
      "Pruning layer model.layers.12.self_attn.k_proj ...\n",
      "time 0.54\n",
      "error 562.7386474609375\n",
      "Pruning layer model.layers.12.self_attn.v_proj ...\n",
      "time 0.53\n",
      "error 64.39682006835938\n",
      "Pruning layer model.layers.12.self_attn.o_proj ...\n",
      "time 0.55\n",
      "error 9.945713996887207\n",
      "Pruning layer model.layers.12.mlp.gate_proj ...\n",
      "time 0.54\n",
      "error 2213.07763671875\n",
      "Pruning layer model.layers.12.mlp.up_proj ...\n",
      "time 0.55\n",
      "error 1556.004638671875\n",
      "Pruning layer model.layers.12.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 19.105846405029297\n",
      "Pruning layer model.layers.13.self_attn.q_proj ...\n",
      "time 0.67\n",
      "error 1602.69482421875\n",
      "Pruning layer model.layers.13.self_attn.k_proj ...\n",
      "time 0.74\n",
      "error 657.2449951171875\n",
      "Pruning layer model.layers.13.self_attn.v_proj ...\n",
      "time 0.82\n",
      "error 55.685752868652344\n",
      "Pruning layer model.layers.13.self_attn.o_proj ...\n",
      "time 0.60\n",
      "error 12.273777961730957\n",
      "Pruning layer model.layers.13.mlp.gate_proj ...\n",
      "time 0.56\n",
      "error 2353.98974609375\n",
      "Pruning layer model.layers.13.mlp.up_proj ...\n",
      "time 0.55\n",
      "error 1706.05615234375\n",
      "Pruning layer model.layers.13.mlp.down_proj ...\n",
      "time 1.62\n",
      "error 23.758045196533203\n",
      "Pruning layer model.layers.14.self_attn.q_proj ...\n",
      "time 0.52\n",
      "error 1353.104248046875\n",
      "Pruning layer model.layers.14.self_attn.k_proj ...\n",
      "time 0.87\n",
      "error 572.6255493164062\n",
      "Pruning layer model.layers.14.self_attn.v_proj ...\n",
      "time 2.07\n",
      "error 56.53020477294922\n",
      "Pruning layer model.layers.14.self_attn.o_proj ...\n",
      "time 0.89\n",
      "error 17.34994125366211\n",
      "Pruning layer model.layers.14.mlp.gate_proj ...\n",
      "time 0.56\n",
      "error 2486.5107421875\n",
      "Pruning layer model.layers.14.mlp.up_proj ...\n",
      "time 0.55\n",
      "error 1879.68115234375\n",
      "Pruning layer model.layers.14.mlp.down_proj ...\n",
      "time 1.83\n",
      "error 29.91707992553711\n",
      "Pruning layer model.layers.15.self_attn.q_proj ...\n",
      "time 0.72\n",
      "error 1613.918701171875\n",
      "Pruning layer model.layers.15.self_attn.k_proj ...\n",
      "time 0.76\n",
      "error 599.1175537109375\n",
      "Pruning layer model.layers.15.self_attn.v_proj ...\n",
      "time 0.55\n",
      "error 73.24479675292969\n",
      "Pruning layer model.layers.15.self_attn.o_proj ...\n",
      "time 0.55\n",
      "error 18.72879981994629\n",
      "Pruning layer model.layers.15.mlp.gate_proj ...\n",
      "time 0.56\n",
      "error 2870.599609375\n",
      "Pruning layer model.layers.15.mlp.up_proj ...\n",
      "time 0.55\n",
      "error 2220.40576171875\n",
      "Pruning layer model.layers.15.mlp.down_proj ...\n",
      "time 1.61\n",
      "error 46.150917053222656\n",
      "Pruning layer model.layers.16.self_attn.q_proj ...\n",
      "time 0.53\n",
      "error 1753.711669921875\n",
      "Pruning layer model.layers.16.self_attn.k_proj ...\n",
      "time 0.53\n",
      "error 676.269287109375\n",
      "Pruning layer model.layers.16.self_attn.v_proj ...\n",
      "time 0.56\n",
      "error 92.20113372802734\n",
      "Pruning layer model.layers.16.self_attn.o_proj ...\n",
      "time 0.54\n",
      "error 17.640966415405273\n",
      "Pruning layer model.layers.16.mlp.gate_proj ...\n",
      "time 0.55\n",
      "error 3735.520263671875\n",
      "Pruning layer model.layers.16.mlp.up_proj ...\n",
      "time 0.54\n",
      "error 2740.3642578125\n",
      "Pruning layer model.layers.16.mlp.down_proj ...\n",
      "time 1.60\n",
      "error 71.23283386230469\n",
      "Pruning layer model.layers.17.self_attn.q_proj ...\n",
      "time 0.54\n",
      "error 1661.8829345703125\n",
      "Pruning layer model.layers.17.self_attn.k_proj ...\n",
      "time 0.65\n",
      "error 627.2976684570312\n",
      "Pruning layer model.layers.17.self_attn.v_proj ...\n",
      "time 0.73\n",
      "error 151.64596557617188\n",
      "Pruning layer model.layers.17.self_attn.o_proj ...\n",
      "time 0.78\n",
      "error 35.631141662597656\n",
      "Pruning layer model.layers.17.mlp.gate_proj ...\n",
      "time 0.65\n",
      "error 4489.8212890625\n",
      "Pruning layer model.layers.17.mlp.up_proj ...\n",
      "time 0.56\n",
      "error 3348.514404296875\n",
      "Pruning layer model.layers.17.mlp.down_proj ...\n",
      "time 3.69\n",
      "error 95.44694519042969\n",
      "Pruning layer model.layers.18.self_attn.q_proj ...\n",
      "time 0.64\n",
      "error 1878.79638671875\n",
      "Pruning layer model.layers.18.self_attn.k_proj ...\n",
      "time 0.56\n",
      "error 637.4218139648438\n",
      "Pruning layer model.layers.18.self_attn.v_proj ...\n",
      "time 0.55\n",
      "error 167.01229858398438\n",
      "Pruning layer model.layers.18.self_attn.o_proj ...\n",
      "time 0.57\n",
      "error 37.015995025634766\n",
      "Pruning layer model.layers.18.mlp.gate_proj ...\n",
      "time 0.54\n",
      "error 5565.22314453125\n",
      "Pruning layer model.layers.18.mlp.up_proj ...\n",
      "time 0.54\n",
      "error 4233.603515625\n",
      "Pruning layer model.layers.18.mlp.down_proj ...\n",
      "time 1.60\n",
      "error 150.0185546875\n",
      "Pruning layer model.layers.19.self_attn.q_proj ...\n",
      "time 0.73\n",
      "error 1758.06787109375\n",
      "Pruning layer model.layers.19.self_attn.k_proj ...\n",
      "time 0.71\n",
      "error 606.147705078125\n",
      "Pruning layer model.layers.19.self_attn.v_proj ...\n",
      "time 0.80\n",
      "error 224.76568603515625\n",
      "Pruning layer model.layers.19.self_attn.o_proj ...\n",
      "time 0.54\n",
      "error 53.38330841064453\n",
      "Pruning layer model.layers.19.mlp.gate_proj ...\n",
      "time 0.58\n",
      "error 6607.7021484375\n",
      "Pruning layer model.layers.19.mlp.up_proj ...\n",
      "time 0.53\n",
      "error 5203.8662109375\n",
      "Pruning layer model.layers.19.mlp.down_proj ...\n",
      "time 1.60\n",
      "error 238.02452087402344\n",
      "Pruning layer model.layers.20.self_attn.q_proj ...\n",
      "time 0.53\n",
      "error 1934.5126953125\n",
      "Pruning layer model.layers.20.self_attn.k_proj ...\n",
      "time 0.55\n",
      "error 682.0623168945312\n",
      "Pruning layer model.layers.20.self_attn.v_proj ...\n",
      "time 0.53\n",
      "error 263.5635986328125\n",
      "Pruning layer model.layers.20.self_attn.o_proj ...\n",
      "time 0.53\n",
      "error 76.74005126953125\n",
      "Pruning layer model.layers.20.mlp.gate_proj ...\n",
      "time 0.55\n",
      "error 7469.3720703125\n",
      "Pruning layer model.layers.20.mlp.up_proj ...\n",
      "time 0.54\n",
      "error 6084.4970703125\n",
      "Pruning layer model.layers.20.mlp.down_proj ...\n",
      "time 1.62\n",
      "error 371.494140625\n",
      "Pruning layer model.layers.21.self_attn.q_proj ...\n",
      "time 0.52\n",
      "error 2050.631591796875\n",
      "Pruning layer model.layers.21.self_attn.k_proj ...\n",
      "time 0.56\n",
      "error 765.3840942382812\n",
      "Pruning layer model.layers.21.self_attn.v_proj ...\n",
      "time 0.61\n",
      "error 366.91015625\n",
      "Pruning layer model.layers.21.self_attn.o_proj ...\n",
      "time 0.73\n",
      "error 139.28770446777344\n",
      "Pruning layer model.layers.21.mlp.gate_proj ...\n",
      "time 0.75\n",
      "error 8886.828125\n",
      "Pruning layer model.layers.21.mlp.up_proj ...\n",
      "time 0.68\n",
      "error 6104.5341796875\n",
      "Pruning layer model.layers.21.mlp.down_proj ...\n",
      "time 1.60\n",
      "error 497.4998779296875\n",
      "Pruning layer lm_head ...\n",
      "time 0.84\n",
      "error 626086.375\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and name in activation_dict:\n",
    "        print(f\"Pruning layer {name} ...\")\n",
    "        gpt = SparseGPT(module)\n",
    "        for activation in activation_dict[name]:\n",
    "            gpt.add_batch(activation.to(next(module.parameters()).device), out=None)\n",
    "        gpt.fasterprune(sparsity=0.5)\n",
    "        gpt.free()\n",
    "        del gpt\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTQbLtE5ZByi",
    "outputId": "ac14f193-d598-4d8d-9026-404f5667ff2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRUNED] ROUGE-1 F1 avg: 0.262\n",
      "[PRUNED] ROUGE-2 F1 avg: 0.112\n",
      "[PRUNED] ROUGE-L F1 avg: 0.170\n",
      "ARTICLE: Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try h ...\n",
      "REFERENCE: Jarryd Hayne quit the NRL in October to try and get into American Football .\n",
      "This week, he signed a three-year contract with the San Francisco 49ers .\n",
      "The chairman of the US Association of Rugby League welcomed his arrival .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try his luck in American football and was this week given a three-year contract with the San Francisco 49ers. Peter Illfield, chairman of US Association of Rugby League, said: 'Jarryd, at 27, is one of the most gifted and talented rugby league players in Australia. He is an extraordinary athlete. Jarryd Hayne (right) has signed with the San Francisco 49ers after quitting the NRL in October . Hayne, who played rugby league for Australia, has signed a three year contract with the 49ers . 'His three-year deal with the 49ers, as an expected running back, gives the USA Rugby League a connection with the American football lover like never before. 'Jarryd's profile and playing ability will bring our sport to the attention of many. It also has the possibility of showing the American college athlete the possibilities of transition and adaptation for them to play in the NFL. 'The 49ers have a strong rugby league background and will be able to help Jarryd to understand the game in the NFL. 'The 49ers have a strong rugby league background and will be able to help Jarryd to understand the game in the NFL. '\n",
      "---\n",
      "ARTICLE: An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other t ...\n",
      "REFERENCE: Faith March's dropped to just five stone as she suffered from anorexia .\n",
      "The 18-year-old from Essex was living on just coffee and no food .\n",
      "After she collapsed in the bathroom, she had hospital treatment .\n",
      "Has now launched a patisserie business to help her recover .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "An anorexic teenager whose weight dropped to just five stone is fighting back from the condition by setting up a catering business. Faith March, 18 from Maldon, Essex, was surviving on nothing other than coffee when she dropped to her lowest weight in March of last year. After several ill-fated attempts to fight the illness, Faith collapsed in her bathroom where she was found by her boyfriend - and her family told her they feared for her life if she didn't get help. Scroll down for video . Faith March's weight dropped to just five stone when she was suffering from anorexia (left) but she is now in recovery and has set up her own patisserie business (right) After treatment at the Priory Hospital in Chelmsford, Faith is now at a healthier weight and credits the starting of her patisserie business, Whisk of Faith, as kick-starting her recovery. Faith said: 'This business has helped me get out of a massive hole. If I'm honest, it was a hole I never thought I would get out of. It's a great way to get out of it. I've got a great team and I've got a great team of people. I've got a great team of people and I've got a great team of people. I've got a great team of people and I've got a\n",
      "---\n",
      "ARTICLE: (CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even  ...\n",
      "REFERENCE: The fan favorite comedy \"Community\" returns for a sixth season on Yahoo .\n",
      "The series is just as weirdly hilarious as ever, with surprises in store .\n",
      "Critics and fans loved the premiere .\n",
      "GENERATED: Summarize the following article:\n",
      "\n",
      "(CNN)For years, they've wanted six seasons and a movie, and at 3:01 a.m. ET Tuesday, fans got it ... almost. There's no movie yet, but \"Community's\" much-awaited sixth season made its debut. Not even cancellation by NBC could kill the fan favorite sitcom, as Yahoo streamed the first two episodes of the new season early Tuesday. The show appears to have suffered no decline in quality in the move to Yahoo, though cast member Yvette Nicole Brown (now on CBS' \"The Odd Couple\") had to leave the show for family reasons. The premiere briefly touches on this in its own way, with Shirley's absence sounding a lot like the setup for a spinoff (as the character Abed points out, along with several other inconsistencies throughout the episode). The \"new Shirley,\" as Dean joked, is Paget Brewster's Frankie, who exists to play the disapproving authority figure to antagonize the former study group. Soon, the group was running a study group, and the new Shirley was a bit of a surprise. The new season is a bit more of a surprise, too, as the show's creator, Chima, revealed the new season will be a \"reunion\" of sorts, with the cast and crew all coming together for\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "articles = df[\"document\"].astype(str).tolist()\n",
    "reference_summaries = df[\"summary\"].astype(str).tolist()\n",
    "generated_summaries = []\n",
    "\n",
    "for article in articles:\n",
    "    prompt = f\"Summarize the following article:\\n\\n{article}\\n\\nSummary:\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(**input_ids, max_new_tokens=64)\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if \"Summary:\" in summary:\n",
    "        summary = summary.split(\"Summary:\")[-1].strip()\n",
    "    generated_summaries.append(summary)\n",
    "    del input_ids, output_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = [scorer.score(ref, gen) for ref, gen in zip(reference_summaries, generated_summaries)]\n",
    "\n",
    "rouge1_avg = sum([s['rouge1'].fmeasure for s in scores]) / len(scores)\n",
    "rouge2_avg = sum([s['rouge2'].fmeasure for s in scores]) / len(scores)\n",
    "rougeL_avg = sum([s['rougeL'].fmeasure for s in scores]) / len(scores)\n",
    "\n",
    "print(f\"[PRUNED] ROUGE-1 F1 avg: {rouge1_avg:.3f}\")\n",
    "print(f\"[PRUNED] ROUGE-2 F1 avg: {rouge2_avg:.3f}\")\n",
    "print(f\"[PRUNED] ROUGE-L F1 avg: {rougeL_avg:.3f}\")\n",
    "\n",
    "for i in range(min(3, len(articles))):\n",
    "    print(\"ARTICLE:\", articles[i][:200], \"...\")\n",
    "    print(\"REFERENCE:\", reference_summaries[i])\n",
    "    print(\"GENERATED:\", generated_summaries[i])\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aP4HXXbcZhfa",
    "outputId": "38150309-7665-4e0b-8236-04439ca4a0bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pruned model and tokenizer saved to: pruned_tinyllama\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "save_dir = \"pruned_tinyllama\"\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"✅ Pruned model and tokenizer saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMy4n72nahq9"
   },
   "source": [
    "Prepare Dataset for HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRD7XyoXaZ_N"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, source_col=\"document\", target_col=\"summary\", max_input=256, max_target=64):\n",
    "        self.inputs = df[source_col].astype(str).tolist()\n",
    "        self.targets = df[target_col].astype(str).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input = max_input\n",
    "        self.max_target = max_target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = f\"Summarize the following article:\\n\\n{self.inputs[idx]}\\n\\nSummary:\"\n",
    "        source = self.tokenizer(prompt, max_length=self.max_input, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        target = self.tokenizer(self.targets[idx], max_length=self.max_target, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        item = {k: v.squeeze() for k, v in source.items()}\n",
    "        item[\"labels\"] = target[\"input_ids\"].squeeze()\n",
    "        return item\n",
    "\n",
    "train_dataset = SummarizationDataset(df, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fRw3tqNasPa"
   },
   "source": [
    "Fine-Tune in the next colab file with LORA"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
